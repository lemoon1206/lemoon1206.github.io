<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/2025/09/01/%E7%94%B5%E5%AD%90%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF%E6%8C%87%E5%8D%97/"/>
    <url>/2025/09/01/%E7%94%B5%E5%AD%90%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<p>电子学习路线指南</p><p>1、UBC ELEC 291&#x2F;292 201 2020W</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250829220529868.png" alt="image-20250829220529868"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2025/08/15/%E5%85%89%E7%94%B5%E8%BD%AC%E4%B8%93%E4%B8%9A%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/"/>
    <url>/2025/08/15/%E5%85%89%E7%94%B5%E8%BD%AC%E4%B8%93%E4%B8%9A%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/</url>
    
    <content type="html"><![CDATA[<p>光电转专业面试</p><p>问：</p><p>1、光学惯性与传感技术</p><p>2、纳米光子学技术</p><p>光学实验</p><p>1、牛顿三棱镜实验</p><p>2、海市蜃楼如何展现</p><p>3、如何展现光在介质中的传播</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250815193850337.png" alt="image-20250815193850337"></p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250815194138066.png" alt="image-20250815194138066"></p><p>准备：</p><p>1、诺贝尔奖</p><p>2、<img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250814225215095.png" alt="image-20250814225215095"></p><p>3、可以去看看光电导前半学期的讲座以及在院网看看相关老师的研究方向（尤其是可能负责转专业面试的老师</p><p>英文问题：</p><p>1、大学最难忘的课程</p><p>2、最喜欢的电影</p><p>3、</p><p>中文问题：</p><p>1、为什么选择光电</p><p>2、最喜欢的颜色？540nm波长的绿光在可见光中差不多在中间表示平衡？</p><p>3、MZI阵列（maybe）是什么</p><p>4、如果信号在传输的时候有一个噪声向量，做线性变换的时候不是会有一个矩阵乘上信号的矩阵嘛，那么现在多出一个噪声向量，请问最坏的情况是什么</p><p>5、自动驾驶有用激光雷达的，有用超声波的，请分别分析他们的优劣，并且如果你是自动驾驶的技术顾问，会如何设计汽车（maybe）</p><p>lz回答的是光学摄像头更为精确但是碰到云雾天气就GG，超声波雷达虽然受天气影响较小，但是波长长，易受衍射极限的干扰成像不清楚，如果我是专家，肯定要将两者结合起来，任何一项实用产品都不会用单一的科技的。另外我还会在每辆车上加一个扫面仪扫描道路情况做一个道路地图，这样所有车都能像有轨电车一样开了，不会掉到河里（我居然还真这么说了）</p><p><img src="http://www-cc98-org-s.webvpn.zju.edu.cn:8001/static/images/ac/02.png" alt="img"></p><p>，然后再去考虑实时应对突发情况。 </p><p>6、如何<strong>设计盲人眼镜</strong>？</p><p>7、<strong>问光电和人工智能的关系</strong>，光电如何促进人工智能发展，人工智能如何促进光电发展</p><p>人工智能可以帮助光电信号传输的时候去除噪声干扰，也可以用在民用的传感器上使图像更加清晰，光电作用于人工智能就是可以利用光的并行性，和干涉做矩阵运算，而非像现在电信号一样线性运行计算，使得算力增强，而且光子器件功耗更低，传输信号更快，在摩尔定律不适用的当下有很大潜力</p><ul><li><p>没有高灵敏度CMOS图像传感器（光电技术），计算机视觉（CV）无法获取高质量输入数据；</p></li><li><p>没有AI算法（如CNN），光电成像系统难以从散射介质中重建清晰图像。</p></li><li><h3 id="二、-技术协同的四大方向"><a href="#二、-技术协同的四大方向" class="headerlink" title="二、 技术协同的四大方向"></a>二、 <strong>技术协同的四大方向</strong></h3><h4 id="1-智能光学感知：突破物理极限"><a href="#1-智能光学感知：突破物理极限" class="headerlink" title="1. 智能光学感知：突破物理极限"></a>1. <strong>智能光学感知：突破物理极限</strong></h4><ul><li><strong>传统瓶颈</strong>：光学衍射极限、信噪比、散射干扰。</li><li><strong>AI赋能</strong>：<ul><li>超分辨率重建：深度学习从低分辨率图像中恢复亚像素细节（如Facebook的RAISR算法）。</li><li><strong>穿透散射介质</strong>：维也纳工业大学利用神经网络实现透过生物组织成像，误差较物理极限仅差0.5%。</li><li><strong>多光谱融合</strong>：结合高光谱相机与AI识别物质成分（如农业病虫害检测）。</li></ul></li></ul><h4 id="2-光电计算架构：重构AI硬件"><a href="#2-光电计算架构：重构AI硬件" class="headerlink" title="2. 光电计算架构：重构AI硬件"></a>2. <strong>光电计算架构：重构AI硬件</strong></h4><ul><li><p><strong>电子芯片瓶颈</strong>：冯·诺依曼架构的“内存墙”、功耗限制。</p></li><li><p><strong>光子芯片突破</strong>：</p><table><thead><tr><th align="left"><strong>技术路线</strong></th><th align="left"><strong>原理</strong></th><th align="left"><strong>AI加速场景</strong></th></tr></thead><tbody><tr><td align="left"><strong>硅基光子计算</strong></td><td align="left">MZI干涉阵列实现矩阵乘法</td><td align="left">大模型推理（能耗降低90%）</td></tr><tr><td align="left"><strong>衍射光学网络</strong></td><td align="left">光衍射实现卷积操作</td><td align="left">边缘端实时目标检测</td></tr><tr><td align="left"><strong>存内光计算</strong></td><td align="left">相变材料直接存储权重</td><td align="left">脉冲神经网络（SNN）训练</td></tr></tbody></table></li></ul><blockquote><p><strong>案例</strong>：清华大学研发的衍射光电芯片，在MNIST识别任务中比GPU快1000倍，能耗仅1%。</p></blockquote><h4 id="3-智能光学设计：颠覆传统流程"><a href="#3-智能光学设计：颠覆传统流程" class="headerlink" title="3. 智能光学设计：颠覆传统流程"></a>3. <strong>智能光学设计：颠覆传统流程</strong></h4><ul><li><strong>传统方式</strong>：人工试错（耗时数月）。</li><li><strong>AI驱动设计</strong>：<ul><li><strong>生成式模型</strong>：输入目标光学响应（如宽带消色差），GAN直接生成微纳结构（效率提升百倍）。</li><li><strong>物理约束嵌入</strong>：PINN（物理信息神经网络）确保设计符合麦克斯韦方程组，避免“物理不可行解”。</li><li><strong>应用</strong>：超构透镜、量子点发光器件、光子晶体光纤。</li></ul></li></ul><h4 id="4-光电系统智能控制"><a href="#4-光电系统智能控制" class="headerlink" title="4. 光电系统智能控制"></a>4. <strong>光电系统智能控制</strong></h4><ul><li><strong>动态可重构器件</strong>：<ul><li>AI实时调控液晶超表面相位，实现自适应波束赋形（6G通信）。</li><li>强化学习优化激光加工参数（切割精度提升40%）。</li></ul></li><li><strong>自主光学系统</strong>：<ul><li>空间望远镜通过AI校正大气湍流（如James Webb望远镜的智能调焦）</li></ul></li></ul></li></ul><p>光电信息科学与工程与AI的关系已超越“工具应用”，正在演化为 <strong>“光为体，智为用”的协同进化生态</strong>：</p><ul><li><strong>光电</strong>提供<strong>超高速、低功耗、高带宽</strong>的物理基础；</li><li><strong>AI</strong> 赋予<strong>感知、决策、创造</strong>的智能内核。</li></ul><p>8、<strong>光谱学中怎么把不同波长的光分开来</strong></p><p>其实大家都想到了三棱镜，但是大家口中说的都是光栅、薄膜干涉、光电晶体管</p><p>9、陀螺 在失去方向的时候如何判断方向？光纤陀螺仪  一束和另一束</p><p>10.<img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250816110924161.png" alt="image-20250816110924161"></p><p>电气：</p><p>老师：盛况 杨永恒</p><p>0.1&#x2F;0.2去电网</p><p>很大部分去电力电子器件（40w）</p><p>比亚迪 极客 车企电机&#x2F;芯片</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2025/08/13/%E7%A7%91%E7%A0%94/"/>
    <url>/2025/08/13/%E7%A7%91%E7%A0%94/</url>
    
    <content type="html"><![CDATA[<p>一些组：</p><p>1、清华类脑计算研究中心</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250813143921380.png" alt="image-20250813143921380"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2025/08/13/%E5%8D%B7/"/>
    <url>/2025/08/13/%E5%8D%B7/</url>
    
    <content type="html"><![CDATA[<p>一些可以卷的方向</p><p>战队：</p><p>helloworld</p><p>超算队 ZJUSCT</p><p>RoboCup仿人机器人战队 ZJU Dancer</p><p>比赛：</p><p>1、世界大学生超级计算机竞赛</p><p>2、RoboCup亚太邀请赛仿人足球机器人比赛</p><p>3、国际大学生超级计算机竞赛</p><p>4、浙江省大学生工程创新竞赛</p><p>5、CMU RI intern</p><p>6、Astar I2R intern</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>UCB cs61a</title>
    <link href="/2025/08/10/UCB%20cs61a/"/>
    <url>/2025/08/10/UCB%20cs61a/</url>
    
    <content type="html"><![CDATA[<p>1、2025春夏python学习资料</p><p><a href="https://py24.fr.to/">2025 年 Python 春夏学习资料</a></p><p>2、进度条：</p><p>2025.8.9 完成了Functions 和HW01</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>机器学习吴恩达</title>
    <link href="/2025/08/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    <url>/2025/08/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE/</url>
    
    <content type="html"><![CDATA[<h2 id="1、进度条："><a href="#1、进度条：" class="headerlink" title="1、进度条："></a>1、进度条：</h2><p>2025.8.10：监督学习 非监督学习 线性回归模型</p><p>2025.8.11：摸了</p><p>2025.8.12：看完线性回归算法</p><h2 id="2、监督学习"><a href="#2、监督学习" class="headerlink" title="2、监督学习"></a>2、监督学习</h2><p>part1:回归 part2:分类</p><h2 id="3、非监督学习"><a href="#3、非监督学习" class="headerlink" title="3、非监督学习"></a>3、非监督学习</h2><p>（1）clustering 聚类</p><p>（2）降维</p><p>（3）异常检测</p><h2 id="4、线性回归模型"><a href="#4、线性回归模型" class="headerlink" title="4、线性回归模型"></a>4、线性回归模型</h2><p>1、找w,b</p><p>利用残差平方和去实现误差最小</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250810215112294.png" alt="image-20250810215112294"></p><p>2、gradient descent algorithm梯度下降</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250811222658707.png" alt="image-20250811222658707"></p><p>学习率大：可能振荡</p><p>学习率小：迭代次数多</p><p>3、线性回归算法</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250812195424119.png" alt="image-20250812195424119"></p><p>推导：复合函数求导（居然真的觉得微积分有实际用途了）</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250812195709274.png" alt="image-20250812195709274"></p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>web开发</title>
    <link href="/2025/08/04/Web%E5%BC%80%E5%8F%91/"/>
    <url>/2025/08/04/Web%E5%BC%80%E5%8F%91/</url>
    
    <content type="html"><![CDATA[<p>Web开发</p><p>进度条：</p><p>2025.8.9草率完结</p><p>速通了一下前端后端的内容</p><p>下面放一些资源</p><p>MIT web development course</p><p><a href="https://elo.mit.edu/iap/">IAP – MIT ELO</a></p><p>课程视频：<a href="https://www.youtube.com/">https://www.youtube.com/</a></p><p>课程网站：<a href="https://weblab.mit.edu/">https://weblab.mit.edu/</a></p><p>作业：<a href="https://github.com/weblab-workshops">weblab-workshops</a></p><p>javascript练习网站：<a href="https://www.jschallenger.com/javascript-practice">https://www.jschallenger.com/javascript-practice</a></p><p>piazza:<a href="https://piazza.com/">https://piazza.com/</a></p><p>csstricks网址：weblab.is&#x2F;flex</p><p>Ui设计组件网站：Mantine</p><p>2025</p><p>完成了 2025 的课程。<br>所有的课程视频在 youtube 上都有，所有的作业在课题组的github上也能找到，对非MIT学生很友好。<br>完全没有基础的也可以学（因为作业的每一步都有答案）。如果有点基础的话，可以1.5倍速快速浏览视频然后做课程项目。<br>预计学时：35 小时 - 50 小时</p><p>基本上是一个网站制作速成课。课程介绍的东西都很浅显易懂，包括：</p><ol><li><p>基础 HTML + CSS + Javascript</p></li><li><p>前端 React.js 基础</p></li><li><p>后端 Express.js 简介</p></li><li><p>后端数据库 mongodb + mongoose （mongodb云数据库可以免费注册，使用时记得挂梯子）</p></li><li><p>后端 serverless：Next.js （一个很小的示例项目 bank-statement-viewer）</p></li><li><p>websocket + canvas （做一个类似 agar.io 的多人即时在线游戏）</p></li><li><p>Git 简介(解决方案：Stack Overflow)\</p><p>Good Git practice: <a href="https://learngitbranching.js.org/">Learn Git Branching</a></p><p>Git Cheat sheet: <a href="https://education.github.com/git-cheat-sheet-education.pdf">Cheatsheet</a></p><p>new branch: git checkout -b “”</p><p>merge: git merge “ “</p><p>remote connection : git push &#x2F;git pull</p><p>&#x2F;未提交情况</p><p>(1)git stash (2)do anything (3) git stash pop</p></li><li><p>Google Auth API</p></li></ol><p>2025 课程视频在 <a href="https://www.youtube.com/@mitweblab/videos">https://www.youtube.com/@mitweblab/videos</a> 上可以找到。除了有一个视频录像窗口错误，没录到编程环节，其他视频都没问题。<br>2025 课程的作业（实验项目）在 <a href="https://github.com/weblab-workshops/catbook-react">https://github.com/weblab-workshops/catbook-react</a> 里，git checkout 指定分支即可。<br>特别注意：全程挂梯子可以解决99%的疑难问题。（尤其是连接云端MongoDB、使用 Google Auth API 等场景）</p><p>到课程结束时，学生会实现一个本地网站的前后端，最终效果见：<a href="https://weblab.is/example">https://weblab.is/example</a><br>包括发送评论、使用 Google Auth 登录、云端MongoDB数据库持久化数据、实时聊天室、类似 agar.io 的多人即时在线游戏等。</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Umich EECS 498/598 2019</title>
    <link href="/2025/07/08/2025-07-08-Umich-EECS-498598-2019/"/>
    <url>/2025/07/08/2025-07-08-Umich-EECS-498598-2019/</url>
    
    <content type="html"><![CDATA[<h3 id="课程介绍"><a href="#课程介绍" class="headerlink" title="课程介绍"></a>课程介绍</h3><p>Deep Learning for Computer Vision是一门介绍深度学习在计算机视觉中的应用的课程，本课程中介绍了如何实现、训练和调试自己的神经网络，并详细展示了计算机视觉的前沿研究。课程中还包括一些训练和微调视觉识别任务网络的实用工程技巧。CS231n是斯坦福大学的版本，由于这门课程在网络上最新的版本是2017年比较早，因此我选择了教学大纲基本相同，但有额外扩充内容的另一门课程，密歇根大学的EECS498&#x2F;598，这门课在网上公开的最新版本是FA2019。</p><p><a href="https://blog.csdn.net/julialove102123/category_12545409.html"><img src="https://g.csdnimg.cn/static/logo/favicon32.ico" alt="img">斯坦福CS231N：面向视觉识别的卷积神经网络（23）_朱晓霞AI的博客-CSDN博客</a></p><p><a href="https://blog.csdn.net/daduzimama/category_12658243.html?spm=1001.2014.3001.5515"><img src="https://g.csdnimg.cn/static/logo/favicon32.ico" alt="img">Deep Learning_松下J27的博客-CSDN博客</a></p><h3 id="Lecture-2"><a href="#Lecture-2" class="headerlink" title="Lecture 2"></a>Lecture 2</h3><p>Lecture 2首先使用了猫作为例子来阐述图片分类任务有哪些挑战，这个例子非常令人印象深刻，我看过的一些博客文章里，也引用了这个“CS231n的猫”的例子。</p><p>图片分类任务的挑战主要包括：</p><ol><li><p>所谓的Semantic Gap，我看了一下，大致的意思就是说人类可以一眼看出来图片上是个猫，但是在计算机眼中，图片只是一堆数字组成的点阵，而且人类没法设计一个精确的算法告诉计算机什么是猫，怎么识别猫。</p></li><li><p>不同的视角：即使是同一只猫，在不同的视角下拍摄的图片也会大不相同。</p></li><li><p>光照：光照也可以是识别的对象发生很大的改变。</p></li><li><p>背景干扰：不同的背景可能会干扰模型的识别结果。</p></li><li><p>Occlusion：有时图片里的猫不会是一个完整的个体，而是有部分被遮挡了。</p></li><li><p>Deformation：物体会发生形变，譬如猫有不同的姿态，站着、躺着、坐着……</p></li><li><p>Intraclass variation：猫的品种不同，它们有不同的颜色、花纹、大小等。给计算机视任务造成了困难。</p></li><li><p>Context：比如说，猫可能与场景中的其他物体有关系，比如猫的身上有栏杆投下的影子，使得猫看起来像一只老虎，我们的算法甚至还需要理解现实世界的一些规律。</p></li></ol><p>要解决上面的问题，可以使用数据驱动（Data-Driven）的算法，这样你无需语义化地告诉计算机什么是猫，只需要提供大量的猫的图片，让计算机自己学习什么是猫。</p><p>这种方法同时带来了另一个好处：<strong>可重用</strong>，也就是说，你使用猫的图片训练模型，模型就能识别猫，使用星系的图片训练模型，模型就能分辨星系的种类。你无需为各种计算机视觉任务开发不同的模型。</p><p><img src="https://www.notion.so/image/attachment%3A1e21ff55-74b6-4798-b028-b6c241dc2075%3Aimage.png?table=block&id=1ed3170b-7f32-8088-b433-c076b912bce9&t=1ed3170b-7f32-8088-b433-c076b912bce9" alt="notion image"></p><h4 id="KNN算法（K-Nearest-Neighbor-Classifier）"><a href="#KNN算法（K-Nearest-Neighbor-Classifier）" class="headerlink" title="KNN算法（K-Nearest Neighbor Classifier）"></a>KNN算法（K-Nearest Neighbor Classifier）</h4><p>KNN算法是一种非常简单的数据驱动算法，它非常容易实现。KNN并不是一个进行图像分类的理想算法（但是经常和其他算法结合，达到更好的效果），课程使用KNN应该是为了以此为例，解释数据驱动的算法。</p><p>KNN的基本思想是：我们首先将训练的图片保存在模型中。给定一张新的图片，我们首先计算这张图片与所有训练图片的距离，然后选择距离最近的K张图片，这K张图片中，出现次数最多的类别，就是这张图片的类别。</p><p>要计算图片之间的距离，首先你要把图片变成一个一维向量，比如一张<code>(H, W, C)</code>的图片（H、W、C分别代表高度、宽度、通道），就可以变成一个<code>(H*W*C)</code>的向量。然后，我们就可以计算任意两张图片之间的距离了。</p><p>距离函数有多种选择，如L1距离（曼哈顿距离）、L2距离（欧式距离）。公式如下：</p><p>d1(I1,I2)&#x3D;∑p∣I1p−I2p∣d2(I1,I2)&#x3D;∑p(I1p−I2p)2<em>d</em>1(<em>I</em>1,<em>I</em>2)&#x3D;<em>p</em>∑∣<em>I</em>1<em>p</em>−<em>I</em>2<em>p</em>∣<em>d</em>2(<em>I</em>1,<em>I</em>2)&#x3D;<em>p</em>∑(<em>I</em>1<em>p</em>−<em>I</em>2<em>p</em>)2</p><p>从图中可以发现，L1距离实现的KNN算法的特点是决策边界都是横线、竖线或45°斜线，而L2距离实现的KNN算法的决策边界可以是任何直线</p><p><img src="https://www.notion.so/image/attachment%3A10c7453a-f0c4-4bb8-a474-b47f0741308c%3Aimage.png?table=block&id=1ed3170b-7f32-8000-9b73-f20b709d3e96&t=1ed3170b-7f32-8000-9b73-f20b709d3e96" alt="notion image"></p><p>像<em>k</em>这样，不是通过模型训练得到，而是提前设置好的参数，被称为<strong>超参数（Hyperparameter）</strong>。</p><h4 id="训练集和测试集（Training-Set-and-Testing-Set）"><a href="#训练集和测试集（Training-Set-and-Testing-Set）" class="headerlink" title="训练集和测试集（Training Set and Testing Set）"></a>训练集和测试集（Training Set and Testing Set）</h4><p>训练集和测试集是两个非常重要的概念。训练集用于训练模型，测试集用于评估模型的性能。</p><p>为什么要分训练集和测试集呢，因为你的模型有可能在训练集上表现很好，但是在测试集上表现不好，这种现象被称为过拟合（Overfitting）</p><p>假如你将所有数据都作为训练集，那么你将无法评估你的模型到底在从未见过的数据集上表现如何。</p><p>具体到KNN算法来说，假如你选取<em>k</em> &#x3D; 1，并使用同一份数据进行训练和测试，那么结果会怎么样？</p><p>答案是你的模型将永远是100%正确，你无法评估模型的效果如何</p><h4 id="交叉验证（Cross-Validation）"><a href="#交叉验证（Cross-Validation）" class="headerlink" title="交叉验证（Cross Validation）"></a>交叉验证（Cross Validation）</h4><p>交叉验证是用来评估模型性能的常用方法。将数据集分成n份，每次选择其中一份作为验证集（Validation set），其他n-1份作为训练集，然后训练模型，最后在测试集上模型的性能。交叉验证可以防止因为分割数据方式不同引起的误差。</p><h4 id="设置超参数"><a href="#设置超参数" class="headerlink" title="设置超参数"></a>设置超参数</h4><p>KNN中有一个参数<em>k</em>，代表选择最近邻的个数。<em>k</em>的取值不同，KNN算法的效果也不同</p><p>我们可以用交叉验证来设置超参数<em>k</em>，我们不能故意选择一个使得模型在测试集上表现很好的<em>k</em>。我们只能在最后使用测试集评估性能，不能人为地引入误差使得模型在测试集上表现很好</p><p>在我们的KNN模型中，我们设置不同的<em>k</em> &#x3D; 1, 2, 3, …，然后在验证集上测试模型的效果，得出了表现最好的<em>k</em> ≈ 7，然后用<em>k</em> &#x3D; 7的模型在整个训练集上训练，最后用测试集评估结果</p><p><img src="https://blog.codecyrus.com/_astro/distance.B9amWHFJ_ZGs8Fy.webp?spaceId=184e2b75-7423-4596-a8f3-d2664bbf3cfa&t=1ae3170b-7f32-81ca-aaf9-fb97bc2c4766" alt="notion image"></p><p>用不同的距离函数实现的KNN算法</p><p><em>按照我的理解，k越小模型越倾向于过拟合，而k过大的话模型容易欠拟合</em></p><p><img src="https://www.notion.so/image/attachment%3Ae3b9680e-3b71-4202-9fc4-30c1e807499c%3Aimage.png?table=block&id=1ed3170b-7f32-8028-a515-c49e36f2df0c&t=1ed3170b-7f32-8028-a515-c49e36f2df0c" alt="notion image"></p><h4 id="KNN的缺陷"><a href="#KNN的缺陷" class="headerlink" title="KNN的缺陷"></a>KNN的缺陷</h4><ul><li><p>图片之间的距离并不能很好的衡量图片的相似程度，比如只是对图片主体进行平移就会使图片距离变得很远。</p></li><li><p>计算的复杂性，课程中提到了<strong>Curse of dimensionality</strong>，假如我们在低维空间内使用足够多的数据点覆盖空间，KNN算法的确能得到不错的效果，然而随着模型维度的升高，我们需要的数据点个数将以指数增加。图像识别任务通常需要使用非常高（成千上万）个维度。假如我们使用4个点覆盖一维坐标轴，那么要达到相似的数据密度，一个平面就需要4242个点，一个10000维的空间就需要410000410000个点！这是无法接受的</p></li><li><p>KNN的训练很快而计算很慢，KNN的训练只需要保存所有数据，如果按照传递引用算就是<em>O</em>(1)，而计算新的数据的分类需要计算它与所有点的距离，这是<em>O</em>(<em>ND</em>)的（<em>N</em>是训练数据个数，<em>D</em>是维度）。这与我们的希望相反，我们更偏爱训练慢而计算快的模型（下一节课就会有这种模型）。</p></li></ul><h3 id="Lecture-3"><a href="#Lecture-3" class="headerlink" title="Lecture 3"></a>Lecture 3</h3><h4 id="线性分类器"><a href="#线性分类器" class="headerlink" title="线性分类器"></a>线性分类器</h4><p>这节课介绍了一个新的模型：<strong>线性分类器（Linear Classifier）</strong>。</p><p>线性分类器就是这样一个简单的模型：</p><p>其中*W*W**是一个权重矩阵，*x*x**是输入的图片。</p><p>这个模型重要不是因为它效果很好，而是因为这个简单的模型是完成后续的复杂模型的“积木”。</p><p>为了理解这个重要的模型，这节课提供了三种视角：</p><p><img src="https://www.notion.so/image/attachment%3A57abf0f4-ab91-436c-990e-af4815e4ea35%3Aimage.png?table=block&id=1ed3170b-7f32-80f3-b9b8-d70b9623c92e&t=1ed3170b-7f32-80f3-b9b8-d70b9623c92e" alt="notion image"></p><h4 id="代数视角"><a href="#代数视角" class="headerlink" title="代数视角"></a>代数视角</h4><p><img src="https://www.notion.so/image/attachment%3A64ff28cf-ffb0-4f38-81a1-5f044d70355a%3Aimage.png?table=block&id=1ed3170b-7f32-80cd-884c-e7e9b3dbbec5&t=1ed3170b-7f32-80cd-884c-e7e9b3dbbec5" alt="notion image"></p><p>从代数视角看，线性分类器仅仅是把图片乘以一个矩阵<em>W</em>，然后加上一个偏置项<em>b</em>，一切操作都是线性的，这意味着将图片乘以一个常数，结果也将线性地变化。对于分类图片，这个性质比较奇怪，但是明白这个有利于我们更好地理解线性分类器。</p><h4 id="可视化视角"><a href="#可视化视角" class="headerlink" title="可视化视角"></a>可视化视角</h4><p><img src="https://www.notion.so/image/attachment%3A27aca702-431e-4862-b385-469170f3e18f%3Aimage.png?table=block&id=1ed3170b-7f32-80f6-91e1-e97fac23c67e&t=1ed3170b-7f32-80f6-91e1-e97fac23c67e" alt="notion image"></p><p>从可视化的视角来看，每一种类别都对应着一个不同的“模版”，我们可以把图片投影到这些模版上，和模版越接近的就会被分到相应的类别。</p><h4 id="几何视角"><a href="#几何视角" class="headerlink" title="几何视角"></a>几何视角</h4><p><img src="https://www.notion.so/image/attachment%3A142b000b-7bc0-4b1e-8e6e-ff5342ad36a7%3Aimage.png?table=block&id=1ed3170b-7f32-8030-9cc8-f1ad794fc409&t=1ed3170b-7f32-8030-9cc8-f1ad794fc409" alt="notion image"></p><p>从几何的视角来看，线性分类器就是一个超平面，面向超平面的方向越往前就越属于某个类别。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>损失函数（Loss Function）是衡量模型预测结果与真实结果的差距的函数。损失越低，意味着模型的预测效果越好。</p><h4 id="SVM-Loss"><a href="#SVM-Loss" class="headerlink" title="SVM Loss"></a>SVM Loss</h4><p>SVM Loss会惩罚错误的分类，和不自信的正确分类。</p><p>总的Loss是所有*Li*L<strong>i</strong>之和</p><h4 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h4><p>正则化可以防止过拟合。正则化向损失函数中加入一个正则化项，惩罚模型中过大的参数：</p><p>其中<em>R</em>(<em>W</em>)是正则化项，<em>λ</em>是正则化系数。</p><p>L2正则化：</p><p>L1正则化：</p><p>Elastic Net：结合使用L1和L2正则化：</p><p>更多方案包括Dropout、Batch Normalization等。</p><p>正则化更喜欢简单的模型，<em>W</em>里面如果有绝对值很大的系数，<em>R</em>(<em>W</em>)就会变大，这使得模型在获得更好的表现的同时采用更简单的方法，避免了过拟合。</p><h4 id="交叉熵（Cross-Entropy）"><a href="#交叉熵（Cross-Entropy）" class="headerlink" title="交叉熵（Cross Entropy）"></a>交叉熵（Cross Entropy）</h4><p>交叉熵将模型的预测结果转换为概率分布，然后计算真实结果的对数似然。</p><p>P(Y&#x3D;k∣X&#x3D;xi)&#x3D;ef(xi,W)∑j&#x3D;1ef(xj,W)<em>P</em>(<em>Y</em>&#x3D;<em>k</em>∣<em>X</em>&#x3D;<em>x**i</em>)&#x3D;∑<em>j</em>&#x3D;1<em>e**f</em>(<em>x**j</em>,<em>W</em>)<em>e**f</em>(<em>x**i</em>,<em>W</em>)</p><p>*Li &#x3D; −log P(Y &#x3D; yi∣X &#x3D; xi)*L<strong>i* &#x3D; −*l</strong>o<em>*g* *P*(<em>Y* &#x3D; *y**i*∣*X</em> &#x3D; *x</em><em>i*)</em></p><h3 id="Lecture-4"><a href="#Lecture-4" class="headerlink" title="Lecture 4"></a>Lecture 4</h3><h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><p>优化（Optimization）是指找到使得损失函数尽量小的模型参数。</p><h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>如果把损失函数看作是一座山脉，如果你要下山，你会每次都沿着高度降低的方向走，这就是梯度下降的思想。</p><p>梯度下降法计算出损失函数在当前参数的梯度，然后沿着<strong>负梯度</strong>的方向<strong>走一小步</strong>，然后更新当前参数，重复这个过程。</p><p>这里写的是负梯度而不是梯度，是因为数学上，梯度的定义是</p><p>这是一个矢量，矢量的方向指向<em>f</em>函数增长最快的方向，而反过来，负梯度的方向就是<em>f</em>函数减小最快的方向。</p><h4 id="数值方法和分析方法"><a href="#数值方法和分析方法" class="headerlink" title="数值方法和分析方法"></a>数值方法和分析方法</h4><p>为了精度和效率的考虑，我们应该使用analytic的方法而不是numerical的方法来计算梯度。像PyTorch这样的深度学习框架都有Autograd的功能，可以方便地自动计算梯度，但有时你也需要手写梯度计算的代码。</p><p>Numerical gradient精度又低计算速度又慢，最好不要使用。例外是gradient check，你可以用数值的方法来验证analytic梯度计算的正确性。这个也不需要你手写，<code>torch.autograd.gradcheck</code>专门用来干这个。</p><h4 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h4><p>学习率（Learning Rate）是梯度下降法中一个重要的超参数。学习率决定了模型参数更新的步长，如果学习率过大，模型可能无法收敛到最优解，如果学习率过小，模型收敛速度会很慢。</p><p>我们上面提到，梯度下降法就是每次向负梯度的方向走<strong>一小步</strong>，学习率决定了这一步有多长。</p><p>其中<em>η</em>是学习率。在训练模型的时候我们会反复应用这一更新规则，直到损失函数下降到可以接受。</p><h4 id="Batch"><a href="#Batch" class="headerlink" title="Batch"></a>Batch</h4><p><em>N</em>是我们训练集的大小，当<em>N</em>比较小的时候，我们可以一次性计算所有梯度。但是当<em>N</em>比较大的时候，一次性计算梯度会是比较昂贵的，（比如显存不够用之类的），这时我们可以分 batch计算梯度,每次只送入一部分数据计算梯度，然后更新参数。</p><p>这就引入了一个新的超参数<code>batch_size</code>。超参数越来越多了，终于有调参炼丹的感觉了（bushi</p><p><code>batch_size</code>的参考取值差不多是32、64、128、256这个大小。</p><h4 id="随机梯度下降（SGD）"><a href="#随机梯度下降（SGD）" class="headerlink" title="随机梯度下降（SGD）"></a>随机梯度下降（SGD）</h4><p>随机梯度下降没什么难理解的，就是每次从训练集中随机sample一个batch来梯度下降。</p><h4 id="SGD-Momentum"><a href="#SGD-Momentum" class="headerlink" title="SGD + Momentum"></a>SGD + Momentum</h4><p>SGD有一些问题：</p><ul><li>Zigzag</li></ul><p>由于SGD每次选择梯度最低的方向，如果某个维度上梯度下降很快，那么其他维度上的步长就会很小，导致算法走出zigzag的形状。</p><p><img src="https://www.notion.so/image/attachment%3Ae2b84c2f-58ff-40c8-8679-4b4966057ac0%3Aimage.png?table=block&id=1ed3170b-7f32-8061-9afc-e01387052705&t=1ed3170b-7f32-8061-9afc-e01387052705" alt="notion image"></p><p>关于图片底部这句话，想要理解它需要一定的线性代数基础，如果不懂的话跳过也没关系。</p><blockquote><p>Aside: Loss function has high condition number: ratio of largest to smallest eigenvalue of Hessian matrix is large.</p></blockquote><p>下面是我个人的一点理解：</p><blockquote><p>Hessian矩阵可以类比成是一元函数的二阶导。直观理解的话，Hessian矩阵可以描述函数在某一点处弯曲的形状。</p><p>矩阵的条件数可以衡量矩阵有多“扁”，比如正交矩阵你就当做是一个圆，它每一个方向上都一样长，它的条件数是1。一个条件数很大的矩阵可以看成一个很扁的椭圆，它最显著的维度和最不显著的维度相差很大。</p><p>Hessian矩阵的条件数很大的话，说明loss funtion在某个维度上面呈现陡峭的“V”形，函数的梯度来回变化，导致算法走出zigzag的形状。</p></blockquote><h4 id="鞍点"><a href="#鞍点" class="headerlink" title="鞍点"></a>鞍点</h4><p>鞍点（Saddle point）就是长这样的点：</p><p><img src="https://www.notion.so/image/attachment%3Ab275e80d-6967-4de7-b505-5c115e7404b8%3Aimage.png?table=block&id=1ed3170b-7f32-8032-b789-c1e757534112&t=1ed3170b-7f32-8032-b789-c1e757534112" alt="notion image"></p><p>鞍点并不是极小值，但是鞍点的梯度却是0，按照之前的算法，零梯度会让我们卡在鞍点动不了。</p><p>顺带一提，Hessian矩阵也可以用来判断鞍点，如果∇<em>f</em>(<strong>x</strong>) &#x3D; 0，且<strong>H</strong>是不定矩阵，即特征值有正有负，那么<strong>x</strong>就是鞍点。（当Hessian矩阵有零特征值的时候这个判断失效，需要展开到三阶或以上）</p><h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>这两个问题都可以通过引入动能项来解决。</p><p><em>这个方程有很多种写法</em>，如果见到了不一样的形式不用奇怪，都是对的</p><p><em>v</em>就好像给了这个点一个速度，让它记住历史的梯度，<em>ρ</em>用来模拟摩擦，让速度衰减，然后每次用梯度更新速度，再用速度更新参数。</p><p>引入动能项可以解决zigzag问题，因为两个相反方向上的梯度会在动能项里相互抵消，让模型专注于其他维度。</p><p>也可以解决鞍点问题，模型走到鞍点的时候还有动能，不会停滞不前。</p><p>一举两得！</p><h4 id="Nestrov-Momentum"><a href="#Nestrov-Momentum" class="headerlink" title="Nestrov Momentum"></a>Nestrov Momentum</h4><p>Mestrov Momentum是SGD + Momentum的变种，它用了“look ahead”的思想，使用<em>xt + ρvt*x<strong>t* + *ρ</strong>v**t*<em>而非</em>xt</em>处的梯度。</p><p><img src="https://www.notion.so/image/attachment%3Ad94a6392-88e4-4584-aacc-72efbb50f887%3Aimage.png?table=block&id=1ed3170b-7f32-80ab-807a-c4846735a05d&t=1ed3170b-7f32-80ab-807a-c4846735a05d" alt="notion image"></p><p>关于这个算法我查阅了相关资料，之所以这样做有更好的效果，是因为这种做法会包含函数的二阶项，允许更加精细的梯度下降（但不是真正的二阶近似）。具体的数学推导比较冗长，有兴趣可以自行了解。</p><h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><p>Adagrad是一种自适应学习率的方法，它会动态调整学习率。当梯度比较平坦的时候走得快一点，梯度比较陡峭的时候走得慢一点。</p><h4 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h4><p>RMSprop是Adagrad的变种，增加了Decay。</p><h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p>Adam≈RMSprop + Momentum。Adam是实践中非常常用，效果也很不错的Optimizer。</p><h4 id="L-BFGS"><a href="#L-BFGS" class="headerlink" title="L-BFGS"></a>L-BFGS</h4><p>到目前为止我们的优化算法都是一阶近似，而BFGS算法使用了二阶近似，<strong>当一次性使用整个训练集做full batch的时候效果非常好</strong>，但是当使用mini-batch的时候效果不怎么好。这比较好理解，因为用到了二阶项，如果batch太小的话，高阶项比低阶项更容易出现偏差。</p><p>缺点是复杂度是<em>O</em>(<em>n</em>3)的，一般只用于小数据集。</p><p>L-BFGS是BFGS的内存优化版本。</p><h4 id="In-practice"><a href="#In-practice" class="headerlink" title="In practice"></a>In practice</h4><ul><li><p><strong>Adam</strong>是一个很好的默认选择，在大多数情况下效果不错。</p></li><li><p><strong>SGD + Momentum</strong>有时比Adam好，但是需要很多玄学调参。</p></li><li><p>如果训练集比较小，你的机器能跑<em>O(n3)*O</em>(*n<em>3)</em>，试试<strong>L-BFGS</strong>吧。</p></li></ul><h3 id="Lecture-5"><a href="#Lecture-5" class="headerlink" title="Lecture 5"></a>Lecture 5</h3><h4 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h4><h4 id="线性分类器的局限"><a href="#线性分类器的局限" class="headerlink" title="线性分类器的局限"></a>线性分类器的局限</h4><p>Linear Classifier能做的其实很有限，从Lecture 3的“几何视角”中，我们知道了线性分类相当于画了一个超平面来将超空间分类。然而并非所有情况下样本点都可以被一个平面简单的分割。</p><p><img src="https://www.notion.so/image/attachment%3A5400f64f-6ec0-4359-9158-0595f3197821%3Aimage.png?table=block&id=1ed3170b-7f32-8073-ab29-d52aa8a8bccb&t=1ed3170b-7f32-8073-ab29-d52aa8a8bccb" alt="notion image"></p><p>一种方法是对原有数据集做一些变换，使得线性分类器能够分割变换后的数据。</p><p><img src="https://www.notion.so/image/attachment%3Ab86d7d74-ab5d-465c-ab01-3554e6a9965a%3Aimage.png?table=block&id=1ed3170b-7f32-80d0-acbf-c72faf9e6b52&t=1ed3170b-7f32-80d0-acbf-c72faf9e6b52" alt="notion image"></p><p>这种方法的确能取得不错的结果，只需要人工提取出数据集的一些特征，然后用这些特征来训练线性分类器。比如，对于图像分类，可以设计算法来提取图像的边缘、颜色、纹理等特征，然后用这些特征来训练线性分类器。同时，也有一些data-driven的方法可以用来提取特征。</p><p>但是这种方法的缺点也很明显，就是研究者必须知道哪一些特征变换对于分类是有效的。而神经网络解决了这个问题。</p><p><img src="https://www.notion.so/image/attachment%3A842b65a7-4ca4-49db-94fc-5924d04672fa%3Aimage.png?table=block&id=1ed3170b-7f32-8025-807b-e0736e2cb4db&t=1ed3170b-7f32-8025-807b-e0736e2cb4db" alt="notion image"></p><p>或许，神经网络只是将特征提取的步骤和训练的步骤融合在了一起。</p><h4 id="从线性分类器到神经网络"><a href="#从线性分类器到神经网络" class="headerlink" title="从线性分类器到神经网络#"></a>从线性分类器到神经网络<a href="https://blog.codecyrus.com/posts/CS231n-EECS598-learning-notes/#%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8%E5%88%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">#</a></h4><p>神经网络其实只是线性分类器的叠加</p><ul><li><p>Linear Score Function: <em>f &#x3D; Wx*f</em> &#x3D; *W<strong>x</strong></p></li><li><p>2-layer Neural Network: <em>f</em> &#x3D; W2σ(W1x) &#x3D; <em>W</em>2<em>σ</em>(<em>W</em>1<em>x</em>)（省略了bias项）</p></li></ul><p>公式里的<em>σ</em>马上就会提到。</p><p><img src="https://www.notion.so/image/attachment%3A633c4a00-9c78-48c7-aeb6-f723e692fe09%3Aimage.png?table=block&id=1ed3170b-7f32-8048-b51a-eb892b22adbb&t=1ed3170b-7f32-8048-b51a-eb892b22adbb" alt="notion image"></p><p>在Lecture 3中，有一种理解线性分类器的视角是把权重矩阵看作是若干个“模版”，而输出代表了图片和每个模版的相似度。</p><p>对于两层的神经网络，我们可以这样理解，把第一个权重矩阵看作是一个“模版仓库”，第二个权重矩阵可以自由组合这些模版，从而得到输出。如此一来，隐含层就代表了图片与每一个模版的相似程度，输出就代表了图片与某种模版的组合的相似程度。</p><p>之前线性分类器中的模版很容易看出到底是什么物体。但是图片左侧这些神经网络第一层的模版，有一些就不好辨认了。按照我的理解，在第一层的网络中，神经网络更倾向于学习更加<strong>基本</strong>的特征，这样才能在后续的层里面充分利用，组合这些特征，完成更加高级的任务。</p><h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>在刚刚的神经网络公式里，有一个<em>σ</em>函数，这个实际上就是神经网络的<strong>激活函数</strong>。</p><p>激活函数对神经网络是<strong>必须</strong>的，假如一个神经网络 <em>f</em> &#x3D; <strong>W2W1</strong><em>x</em> 没有激活函数，会怎么样？没错，他其实就退化成了一个线性分类器 <em>f</em> &#x3D; (<strong>W2W1</strong>)<em>x</em>。我们白费力气地将两个线性分类器堆叠，然后得到了一个新的线性分类器。</p><p>激活函数给一大堆线性的矩阵中间加上了非线性的成分，让神经网络得以拟合非线性的函数。</p><p>常用的激活函数有这一些：</p><p><img src="https://www.notion.so/image/attachment%3A89a37e0b-e5a6-4858-8b29-b97ce45ad44f%3Aimage.png?table=block&id=1ed3170b-7f32-80ac-a33d-c23f4591817f&t=1ed3170b-7f32-80ac-a33d-c23f4591817f" alt="notion image"></p><p>ReLU是深度学习中最常用的激活函数。</p><p>激活函数可以非线性地变换空间，使得线性不可分的点云转化成线性可分的点云。</p><p><img src="https://www.notion.so/image/attachment%3A7eb8f919-06d1-436e-b8d7-eca67bf76073%3Aimage.png?table=block&id=1ed3170b-7f32-80f3-9ac0-f38cc5125aca&t=1ed3170b-7f32-80f3-9ac0-f38cc5125aca" alt="notion image"></p><h4 id="万能近似定理（Universal-Approximation-Theorem）"><a href="#万能近似定理（Universal-Approximation-Theorem）" class="headerlink" title="万能近似定理（Universal Approximation Theorem）#"></a>万能近似定理（Universal Approximation Theorem）<a href="https://blog.codecyrus.com/posts/CS231n-EECS598-learning-notes/#%E4%B8%87%E8%83%BD%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%86universal-approximation-theorem">#</a></h4><blockquote><p>万能近似定理（Universal Approximation Theorem） 单隐含层的神经网络可以拟合任意连续函数f : Rn → Rm<em>f</em> : R<em>n</em> → R<em>m</em></p></blockquote><p>其证明过程考虑了ReLU作为激活函数的情况，首先构造了一个“bump function”，这个函数在某个位置凸起，其他位置为0，通过线性地组合这个函数，可以拟合任何连续函数。</p><p><img src="https://www.notion.so/image/attachment%3A7d572096-d2ef-456b-88c4-3444d1a6d782%3Aimage.png?table=block&id=1ed3170b-7f32-802b-a4d3-ef5ae0f2f043&t=1ed3170b-7f32-802b-a4d3-ef5ae0f2f043" alt="notion image"></p><p>Universal Approximation的确很cool，但其实只是一种数学上的构造，这个定理解决不了什么工程上的问题，如“我们是否能用SGD拟合这个函数”、“我们需要多少数据才能拟合这个函数？”等。它只是告诉我们有这样一组权值可以让神经网络拟合你想要拟合的函数，但是并没有告诉我们如何找到它。</p><p>并且，拟合任意函数看起来是一个很强大的性质，其实不然，简单如K-Means的算法都能做到万能近似。</p><h4 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a>凸优化</h4><p>万能近似定理不能保证我们能否找到一组权值来最佳拟合，但是假如我们的目标函数是凸（convex）的，那么相关的理论可以保证，我们就能保证无论选取什么样的初始值，都能找到全局最优解。</p><p>线性分类器的优化问题就是一个凸优化问题。对于线性分类器，无论你使用SVM loss还是Softmax，L1正则化还是L2正则化，最终的目标函数都是凸的！这意味着线性分类器可以保证一定收敛到全局最优解，线性分类器的优化问题具有确定性。</p><p>然而，很遗憾的是，多层的神经网络并不具有这种性质，神经网络收敛到哪里依赖于随机初始化的值，或是优化的方法，神经网络不一定能收敛到全局最优解，可能陷入局部最优解，甚至不收敛。尽管在工程实践中神经网络取得了许多成功，神经网络的有效性并没有理论的保证，这仍然是一个相当活跃的研究领域。</p><h3 id="Lecture-6"><a href="#Lecture-6" class="headerlink" title="Lecture 6"></a>Lecture 6</h3><h4 id="误差反向传播"><a href="#误差反向传播" class="headerlink" title="误差反向传播"></a>误差反向传播</h4><p>为了优化误差函数，寻找到最优的权值<em>W</em>，我们需要计算梯度∇<em>WL</em>。</p><p>相比手动计算误差函数的梯度，更好的方法是发明一些数据结构和算法来自动计算梯度。</p><p>这里的数据结构就是计算图，计算图用图的方式建模了数据在的计算过程。而算法就是反向传播（Back Propagation），我们可以反向遍历计算图来更新整个模型的梯度。这种方法允许我们使用更复杂、更灵活的模型，而无需推导梯度的表达式。</p><p>计算梯度时，我们首先正向计算输出，然后反向传播梯度。</p><p><img src="https://www.notion.so/image/attachment%3A25fe1ffc-b767-4ae8-a45d-11348bc92230%3Aimage.png?table=block&id=1ed3170b-7f32-80e5-a44a-c3ea064800aa&t=1ed3170b-7f32-80e5-a44a-c3ea064800aa" alt="notion image"></p><p>使用计算图和误差反向传播的另一个好处是<strong>模块化</strong>，我们可以在计算图中定义自己的节点，而不必永远使用最简单的算术原语。</p><p><img src="https://www.notion.so/image/attachment%3Aa87f7026-226d-4a5d-b3d4-46a105d41158%3Aimage.png?table=block&id=1ed3170b-7f32-8065-b5ee-f5efa976aedb&t=1ed3170b-7f32-8065-b5ee-f5efa976aedb" alt="notion image"></p><p>课程提到了一个让我感觉挺新奇的视角，它介绍了这几种门在梯度传播流中的意义。</p><p><img src="https://www.notion.so/image/attachment%3A880995ac-8a5d-4ad3-a4db-a7956055ee57%3Aimage.png?table=block&id=1ed3170b-7f32-8082-a655-daeb3137001c&t=1ed3170b-7f32-8082-a655-daeb3137001c" alt="notion image"></p><ul><li><p>add gate：相加门将下游的梯度复制到上游的每一个输入。</p></li><li><p>copy gate：复制门将下游的梯度相加传到上游。<strong>copy gate和add gate</strong>在某种程度上是<strong>对偶</strong>的。</p></li><li><p>mul gate：乘法门将下游的梯度乘到上游并交换两个输入，mul gate在某种程度上将梯度混合在一起了。</p></li><li><p>max gate：最大门将梯度传播到最大的输入，相当于起到了一个路由的作用。</p></li></ul><p>在神经网络中，我们实际上处理的不是标量，而是向量。道理是相同的，只需要将一元的求导变成对向量求导。整个计算过程是相通的。</p><p><img src="https://www.notion.so/image/attachment%3A9883bd27-5430-48bb-831c-725fd556dde0%3Aimage.png?table=block&id=1ed3170b-7f32-8018-a3c0-ded5d1408c6e&t=1ed3170b-7f32-8018-a3c0-ded5d1408c6e" alt="notion image"></p><p>这里的向量对标量求导很直观。而这里的向量对向量求导，实际上就是<strong>Jacobian矩阵</strong>（然而在实际的梯度计算中并不会显式地形成一个Jacobian矩阵）。</p><p>反向传播不仅能计算一阶的梯度，也能计算更高阶的导数。</p><h3 id="Lecture-7"><a href="#Lecture-7" class="headerlink" title="Lecture 7"></a>Lecture 7</h3><h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><p>在处理图像信息的时候，全连接层存在一个明显的局限性，全连接层会将一个32x32x3的图像，展开成一个3072维的向量，然后经过一个全连接的权矩阵<em>W</em>，计算出输出。但是这样的问题是，<strong>图片中原有的位置信息全部丢失了</strong>。</p><p>卷积层可以解决这个问题。卷积层引入了卷积核，让卷积核在图片上滑动，然后计算卷积核覆盖的部分的带权和，输出到下一层。通过这种方式，卷积层保留了图片的空间信息。卷积核的大小一般是3x3或5x5。一个卷积层会包括不止一个卷积核，而是多个卷积核。</p><p><img src="https://www.notion.so/image/attachment%3A89ffea59-0d95-498f-aabb-500c12e85f15%3Aimage.png?table=block&id=1ed3170b-7f32-8050-b88d-d6f2b6a1dab3&t=1ed3170b-7f32-8050-b88d-d6f2b6a1dab3" alt="notion image"></p><p>convolution layer</p><p>两种理解卷积层的视角：</p><ol><li><p>将卷积层理解成activation maps，一种卷积核对应的输出的每一个位置告诉我们这个位置附近的像素和此卷积核的匹配程度。</p></li><li><p>将卷积层理解成一个特征向量的网格s每个位置有n种卷积核对应的输出，这些输出拼起来就是一个特征向量。</p></li></ol><p>观察卷积层输出的形状，不难发现它也可以看作是图片，因此我们可以将卷积层相连，形成多层的卷积神经网络。</p><p>在连接卷积层的时候，记得一定要加上激活函数，原因和之前的全连接层s一样。如果不加激活函数，两个卷积层串联在一起，其实只是相当于一个卷积层的效果。</p><h4 id="卷积核学习到什么？"><a href="#卷积核学习到什么？" class="headerlink" title="卷积核学习到什么？"></a>卷积核学习到什么？</h4><p>回忆Lecture 3，在理解线性分类器的第二种视角里，我们发现线性分类器的权重实际上学习到了一种图片的“模版”，而卷积核学习到了什么呢？</p><p><img src="https://www.notion.so/image/attachment%3Ae01128d4-4b0f-4708-b099-5d053557a17b%3Aimage.png?table=block&id=1ed3170b-7f32-80a1-8d10-dff424a73073&t=1ed3170b-7f32-80a1-8d10-dff424a73073" alt="notion image"></p><p>实际上，卷积核学习到的是一些局部的特征，如图，AlexNet的第一层是一些简单的条纹和网格纹理。</p><p>结合前面的两种视角，AlexNet第一层的输出，可以理解成图片对64种卷积核的激活地图，也可以理解成图片在每一个位置都有一个64维的特征向量。而这个输出会被AlexNet后面的卷积层使用，后面的卷积层可以利用前面学习到的简单的纹理，并组合它们形成更加复杂的pattern。</p><h4 id="卷积层的超参数"><a href="#卷积层的超参数" class="headerlink" title="卷积层的超参数"></a>卷积层的超参数</h4><p>首先是我们前面就提到的</p><ul><li><p>输入的大小：<em>W × W*W</em> × *W**</p></li><li><p>卷积核的大小：<em>K × K*K</em> × *K**</p></li></ul><p>经过这样的一层卷积之后，图片的大小会变成<em>W − K + 1*W</em> − *K* + 1*</p><h4 id="引入Padding"><a href="#引入Padding" class="headerlink" title="引入Padding"></a>引入Padding</h4><p>注意到我们上面的卷积会使得输出的图像形状变小几个像素，我们通常不希望输出发生这样的变化，最好是保证输入输出的形状保持相同，因此我们可以在图片的四个边缘加上<em>P</em>个像素，用“0”填充。</p><p>这样，输出的图像大小就是 <em>W − K + 1 + 2P*W</em> − *K* + 1 + 2*P**，当 <em>P &#x3D; (K − 1)&#x2F;2*P</em> &#x3D; (*K* − 1)&#x2F;2* 时，输入输出的形状保持一致，这被称为“same padding”，也是最常用的一种设定。</p><p>用“0”填充图片边缘会不会引入额外的信息？Justin Johnson提到，实际上，zero padding隐式地给图片引入了边缘的位置信息。有一些卷积核会通过边缘的“0”学习到如何识别图像边缘。这不好说是一种Bug还是Feature。</p><h4 id="引入Stride-Receptive-Fields-感受野"><a href="#引入Stride-Receptive-Fields-感受野" class="headerlink" title="引入Stride &amp; Receptive Fields 感受野"></a>引入Stride &amp; Receptive Fields 感受野</h4><p><img src="https://www.notion.so/image/attachment%3Ac6968c11-1305-4b65-bb58-f9005498d371%3Aimage.png?table=block&id=1ed3170b-7f32-8018-97d8-d08858fd44cd&t=1ed3170b-7f32-8018-97d8-d08858fd44cd" alt="notion image"></p><p>注意到，假如输入的图片很大，或者卷积核的尺寸很小，我们就需要更多的层数才能让某一点“看到”整张图片。如果我们能让图片的大小缩小一点，就可以加快这个“看到全局”的过程。</p><p>因此我们可以引入一个stride，<em>S</em>。当用卷积核遍历图片的时候，我们让卷积核每次移动的距离为<em>S</em>，这样，输出图片的大小就会被大大缩小。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>超参数：</p><ol><li><p>Input：<em>W*W</em>*</p></li><li><p>Kernel：<em>K*K</em>*</p></li><li><p>Padding：<em>P*P</em>*</p></li><li><p>Stride: <em>S*S</em>*</p></li></ol><p>最终的输出为：(W − K + 2P)&#x2F;S + 1(<em>W</em> − <em>K</em> + 2<em>P</em>)&#x2F;<em>S</em> + 1，我们一般会调整参数让这个式子能挣除。</p><h4 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h4><p>有时候会有卷积核大小为1x1的时候，这种层可以看作是每一个像素形成一个全连接层，通过这种方式可以将每一个位置对应的特征向量降维，有时候也是很有用的。</p><p>其他维度的卷积。我们刚才讨论的都是2D卷积，实际上，也可以进行1D卷积，3D卷积等，道理是一样的。</p><p>PyTorch提供了开箱即用的卷积层<code>torch.nn.Conv2d</code>、<code>torch.nn.Conv1d</code>、<code>torch.nn.Conv3d</code>，只需要对超参数进行配置，就可以往神经网络里加入卷积层。</p><h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p>刚刚提到了可以用Stride &gt; 1，构造一个downsample的卷积层。另一种downsample的方式是使用<strong>池化层</strong>。</p><p>池化层一个好处就是它没有任何需要学习的参数。</p><h4 id="Max-Pooling"><a href="#Max-Pooling" class="headerlink" title="Max Pooling"></a>Max Pooling</h4><p>就是对每个区域计算最大值。</p><p><img src="https://www.notion.so/image/attachment%3A1e663561-82d2-45ff-9b3a-5db0b97fd8ac%3Aimage.png?table=block&id=1ed3170b-7f32-8085-8ca8-ff9b45a932b7&t=1ed3170b-7f32-8085-8ca8-ff9b45a932b7" alt="notion image"></p><h4 id="Average-Pooling"><a href="#Average-Pooling" class="headerlink" title="Average Pooling"></a>Average Pooling</h4><p>就是对每个区域计算平均值。</p><h4 id="搭建神经网络"><a href="#搭建神经网络" class="headerlink" title="搭建神经网络"></a>搭建神经网络</h4><p>有了卷积层、池化层这些乐高积木，我们就可以搭建自己的卷积神经网络了。卷积神经网络通常从前到后，图片的形状维度会越来越小，特征维度会越来越大，形成一个“逐渐铺平”的效果，CNN一个经典的结构是LeNet-5。</p><p><img src="https://www.notion.so/image/attachment%3Ad6ef5b42-d74c-4ba2-9457-360a6ffa1298%3Aimage.png?table=block&id=1ed3170b-7f32-80c5-9d96-ea6b1fa8e8e6&t=1ed3170b-7f32-80c5-9d96-ea6b1fa8e8e6" alt="notion image"></p><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>深度神经网络非常难以训练。</p><p>比起层数少的神经网络，深度神经网络非常难收敛。为了解决这个问题，我们引入normalizatio层。其中最常用的一种是<strong>batch normalization</strong>。</p><p>为什么normalization可以加速深度神经网络的收敛，我查阅了一些资料，以下是我的理解：</p><p>深度神经网络因为层数深，当训练的时候存在“内耗”的问题。因为误差反向传播到中间的某一层的时候，实际上已经叠加了后面所有层的梯度，这一层不是在“配合”我们的正确输出，而是在“配合”之后的所有层。而这种误差会被放大，第i-1层会去拟合第i层，导致数据在流过深度神经网络的时候经历十分“曲折”的旅途，大部分时间不是为了拟合训练数据，而是为了相邻层之间的对齐，造成了一种“内耗”。</p><p>这种现象的一种表现就是“internel covariate shift”，由于神经网络深度比较深，在训练的过程中每一层可能会不再独立同分布，使得某一层需要适应新的输入分布，还可能会造成数值落入激活函数的饱和区，造成梯度消失。batch normalization让输出保持平均值为0，标准差为1的“正态分布”。缓解了这个问题。（这就是统计上常用的“白化”处理）</p><p><img src="https://www.notion.so/image/attachment%3A8a6e689a-c5c5-4826-ab94-597fc303f6ca%3Aimage.png?table=block&id=1ed3170b-7f32-807f-81ab-ec1dfc022f86&t=1ed3170b-7f32-807f-81ab-ec1dfc022f86" alt="notion image"></p><p>但是有时候，这个平均值为0，标准差为1的限制可能太过与严格，为了增加灵活性，增强模型的表达能力，我们可以学习一个scale<em>γ</em>和一个shift<em>β</em>,令<em>yi</em>, <em>j</em> &#x3D; <em>γjxi</em>, <em>j</em> + <em>βj</em>，然后在每个minibatch里面调整这两个参数。不过这带来一个反直觉的问题，就是我们同一个batch里面的不同数据，会互相之间影响，并且在测试模型的时候，我们并没有所谓的minibatch给我们计算参数。</p><p>所以我们可以在测试的时候，把这两个参数直接设置成训练时参数的移动平均。这样还带来了一个好处。由于normalization layer是线性的，因此当参数均为常数的时候，这一层可以合并到上一层，这是zero overhead!</p><p>一般来说，batch normalization layer会放在fully-connected layer或convolution layer后面，激活函数的前面。</p><p>好处：更快收敛、更稳定的随机初始化、test-time zero overhead等。</p><p>坏处：暂未被理论很好地解释，在training和在testing表现不同。</p><h4 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h4><p>以下非课程内容，为个人学习并补充。</p><p>近些年来，batch normalization变得越来越不流行。新的网络更多使用layer normalization。如transformer。</p><p>layer normalization实际上比batch normalization更简单，它只对每个样本进行归一化，不依赖batch size，而且laer normalization在测试与训练表现一致。</p><p>其他的Normalization方法：</p><ul><li><p>Instance Normalization：对每个样本的每个通道独立进行归一化，适合处理图像数据中的风格和内容。</p></li><li><p>Group Normalization：将特征通道分成多个组，对每个组进行归一化处理，综合了 Batch Normalization 和 Layer Normalization 的优点，适合于小batch size 和高维数据。</p></li><li><p>Weight Normalization：对权重矩阵进行归一化，将权重分解为规范化的部分和非规范化的部分，可以加速训练过程，提高模型收敛速度，特别是在深层网络中。</p></li></ul><h3 id="Lecture-8"><a href="#Lecture-8" class="headerlink" title="Lecture 8"></a>Lecture 8</h3><h4 id="AlexNet-2012"><a href="#AlexNet-2012" class="headerlink" title="AlexNet (2012)"></a>AlexNet (2012)</h4><p>AlexNet是一个非常有影响力的卷积神经网络，Google Scholar显示，其原论文的引用已经达到了惊人的160203次。</p><p>AlexNet的结构就是<code>(conv + [pool]) * 5 + (fc + dropout) * 3</code>，随着层数的增加，图片的长宽不断减少，而通道数不断增多。当通道数达到一定数量时，它们会flatten后作为特征输入全连接神经网络中。</p><p>分析内存使用和参数量，可以发现AlexNet有趣的特征，随层数增加，内存占用变少，而参数量增多，即所有内存基本都在前面的卷积层，所有参数量基本都在后面的全连接层。</p><h4 id="ZFNet-2013"><a href="#ZFNet-2013" class="headerlink" title="ZFNet (2013)"></a>ZFNet (2013)</h4><p>ZFNet基本上就是一个更大的AlexNet，但是其基本结构依然是相同的，前面是卷积层和池化层，后面是全连接层。</p><h4 id="VGG-2014"><a href="#VGG-2014" class="headerlink" title="VGG (2014)"></a>VGG (2014)</h4><p>AlexNet和ZFNet的网络结构参数，即卷积核形状, 每层通道数，池化层形状，都是通过尝试人工设置出来的。这让网络的scale up变得困难。VGG提出了一种系统的卷积神经网络设计方法：</p><ol><li><p>所有卷积层都是3x3, 步长为1, 填充为1：两个3x3的卷积层优于一个5x5的卷积层，我们没有理由使用比3x3更大的卷积核。</p></li><li><p>所有池化层就是2x2, 步长为2，使得长宽减半，每经过一个池化层，卷积的通道翻倍。这样的设计使得FLOPs数在每层之间保持相同。</p></li></ol><p><img src="https://www.notion.so/image/attachment%3A1ea7a7a8-17b1-407b-b229-a410ee8b3fd4%3Aimage.png?table=block&id=1ed3170b-7f32-808e-80a3-cb7bedd75a89&t=1ed3170b-7f32-808e-80a3-cb7bedd75a89" alt="notion image"></p><h4 id="GoogleNet-2014"><a href="#GoogleNet-2014" class="headerlink" title="GoogleNet (2014)"></a>GoogleNet (2014)</h4><p>GoogleNet很有趣，我们看到了ZFNet和VGG都开始叠参数量，神经网络变得越来越大。但是GoogleNet是一个关注效率的网络，因为谷歌想要在现实中使用这个模型。</p><p>GoogleNet为了达到这一点，卷积网络的空间维度下降得非常快，只经过了最初的几层，就从224x224降到了28x28。</p><p>GoogleNet还是用了一种叫做Inception的模块，Inception使用多种不同尺寸的卷积核并行，加速了计算效率。</p><p>GoogleNet还使用了一种叫做Global Average Pooling的技术，避免使用全连接层的巨大开销。</p><h4 id="ResNet-2015"><a href="#ResNet-2015" class="headerlink" title="ResNet (2015)"></a>ResNet (2015)</h4><p><img src="https://www.notion.so/image/attachment%3A3356c6d7-9d27-43e2-b533-0ed8fffcd244%3Aimage.png?table=block&id=1ed3170b-7f32-80a8-acb5-f6205c8fa1e3&t=1ed3170b-7f32-80a8-acb5-f6205c8fa1e3" alt="notion image"></p><p>2015年之后，ImageNet获奖者的深度跳跃式地从十几层进化到几百层，发生了什么？</p><p>自从Batch Normalization出现以后，10+层的神经网络变得可行，然而再深度的神经网络却效果不好，反直觉的是五十多层的神经网络反而比二十多层的效果差，理论上来说深层的神经网络应该可以包括浅层的神经网络作为其中一部分，然后剩余的部分只要拟合单位矩阵就可以了。没有道理深层的神经网络更差。</p><p>一开始人们认为这是因为过拟合的问题，后来的一些研究表明，这些网络并非过拟合，而是不知怎么地欠拟合了。也就是说即使使用了batch normalization，现有的优化方式仍然失效。</p><p>上面提到了，如果有一个训练好的浅层神经网络，可以让深层神经网络的前几层和这个浅层神经网络一样，然后剩下的层什么都不做，只要拟合单位函数就可以了。然而实验证明我们连这样简单的策略都很难通过优化器得到。于是就有一个简单的思想，既然你需要单位函数，我就给你单位函数。原本是让某一层拟合<em>y</em> &#x3D; <em>f</em>(<em>x</em>)，现在拟合<em>y</em> &#x3D; <em>f</em>(<em>x</em>) + <em>x</em>，也就是说，如果需要单位函数，<em>f</em>(<em>x</em>)只要始终输出0就行了，这被称为残差链接。</p><p>残差链接可以促进深层神经网络的收敛，让深层神经网络的训练成为可能，这就是ResNet的基本思想。</p><p>就是这么一个非常简单的思想，促成了“Revolution of Depth”。从此之后，神经网络的层数大大增加。</p><p><strong>Reference：</strong><a href="https://cyrus28214.github.io/posts/CS231n-EECS598-learning-notes/">CS231n&#x2F;EECS598学习笔记 - Cyrus’ Blog</a></p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>支教物理教学计划</title>
    <link href="/2025/07/08/%E5%A4%A9%E5%8F%B0%E6%94%AF%E6%95%99%E7%89%A9%E7%90%86%E6%95%99%E5%AD%A6%E8%AE%A1%E5%88%92/"/>
    <url>/2025/07/08/%E5%A4%A9%E5%8F%B0%E6%94%AF%E6%95%99%E7%89%A9%E7%90%86%E6%95%99%E5%AD%A6%E8%AE%A1%E5%88%92/</url>
    
    <content type="html"><![CDATA[<h1 id="物理教学计划"><a href="#物理教学计划" class="headerlink" title="物理教学计划"></a>物理教学计划</h1><h2 id="第一周（7-19——7-25）"><a href="#第一周（7-19——7-25）" class="headerlink" title="第一周（7.19——7.25）"></a>第一周（7.19——7.25）</h2><p><strong>7.19 7.20 模考1</strong></p><ul><li>试题主要为整卷部分+几道热学大题：意在发现学生热学大题薄弱点所在</li></ul><p><strong>7.21 讲解热力学定律和能量守恒</strong></p><ul><li>W、Q、U的计算（结合图像）</li><li>p-V图、T-V图、p-T图的识别</li></ul><p><strong>7.22 常见热学大题的解题思路 1</strong></p><ul><li>液柱或活塞移动问题</li></ul><p><strong>7.23</strong></p><ol><li><p><strong>常见热学大题的解题思路 2</strong></p><ul><li>汽缸问题</li></ul></li><li><p><strong>习题课</strong></p><ul><li>讲解模考试卷和周一周二的作业</li></ul></li></ol><p><strong>7.24 常见热学大题的解题思路 3</strong></p><ul><li>变质量气体问题</li></ul><p><strong>7.25 热学大题</strong></p><ul><li>讲解周三周四作业</li><li>力学大题：力的分类&amp;受力分析（开个头）</li></ul><h2 id="第二周（7-26——8-1）"><a href="#第二周（7-26——8-1）" class="headerlink" title="第二周（7.26——8.1）"></a>第二周（7.26——8.1）</h2><p><strong>7.26 7.27 模考2</strong></p><ul><li>试题内容主要为整卷+几道力学大题：意在发现学生力学大题薄弱点所在</li></ul><p><strong>7.28 力学大题的解题思路1</strong></p><ul><li>碰撞模型</li></ul><p><strong>7.29 力学大题的解题思路2</strong></p><ul><li>人船模型</li></ul><p>**7.30 **</p><ol><li><strong>力学大题的解题思路3</strong><ul><li>传送带问题</li></ul></li><li><strong>习题课</strong><ul><li>讲解模考题以及之前的作业错题</li></ul></li></ol><p><strong>7.31 力学大题的解题思路4</strong></p><ul><li>板块模型（速度型）的讲解</li></ul><p><strong>8.1 力学大题的解题思路5</strong></p><ul><li>讲解之前的作业错题、力学大题做总结</li></ul>]]></content>
    
    
    <categories>
      
      <category>杂</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>支教物理教案</title>
    <link href="/2025/07/08/%E7%89%A9%E7%90%86%E6%95%99%E6%A1%88/"/>
    <url>/2025/07/08/%E7%89%A9%E7%90%86%E6%95%99%E6%A1%88/</url>
    
    <content type="html"><![CDATA[<h1 id="物理教案"><a href="#物理教案" class="headerlink" title="物理教案"></a>物理教案</h1><h2 id="7-21-讲解热力学定律和能量守恒"><a href="#7-21-讲解热力学定律和能量守恒" class="headerlink" title="7.21 讲解热力学定律和能量守恒"></a><strong>7.21 讲解热力学定律和能量守恒</strong></h2><h3 id="讲解概念"><a href="#讲解概念" class="headerlink" title="讲解概念"></a>讲解概念</h3><ol><li><p>系统：研究某一容器中气体的热学性质，其研究对象是容器中的大量分子组成的系统，这在热学中叫作一个热力学系统（thermodynamic system），简称系统。</p></li><li><p>基本状态参量：压强、体积、温度（区分热力学温度与摄氏温度）</p></li><li><p>理想气体的定义：：这种气体分子大小和相互作用力可以忽略不计，也可以不计气体分子与器壁碰撞的动能损失。这样的气体在任何温度、任何压强下都遵从气体实验定律，我们把它叫作理想气体（ideal gas）</p></li><li><p>理想气体状态方程</p><p><img src="C:\Users\王博文\AppData\Roaming\Typora\typora-user-images\image-20250705125544870.png" alt="image-20250705125544870"></p><p>波义尔定律：等温变化</p><p><img src="C:\Users\王博文\AppData\Roaming\Typora\typora-user-images\image-20250705125638313.png" alt="image-20250705125638313"></p><p><img src="C:\Users\王博文\AppData\Roaming\Typora\typora-user-images\image-20250705125620513.png" alt="image-20250705125620513"></p><p>盖—吕萨克定律：等压变化</p><p><img src="C:\Users\王博文\AppData\Roaming\Typora\typora-user-images\image-20250705125729266.png" alt="image-20250705125729266"></p><p><img src="C:\Users\王博文\AppData\Roaming\Typora\typora-user-images\image-20250705125719812.png" alt="image-20250705125719812"></p><p>查理定律：等容变化</p><p><img src="C:\Users\王博文\AppData\Roaming\Typora\typora-user-images\image-20250705125813325.png" alt="image-20250705125813325"></p><p><img src="C:\Users\王博文\AppData\Roaming\Typora\typora-user-images\image-20250705125836287.png" alt="image-20250705125836287"></p><p>介绍绝热过程（Q&#x3D;0）</p></li><li><p>热力学第一定律：对于一个热力学系统而言，一个热力学系统的内能变化量等于外界向它传<br>递的热量与外界对它所做的功的和，这个关系叫作热力学第一定律。</p><p><img src="C:\Users\王博文\AppData\Roaming\Typora\typora-user-images\image-20250705125954022.png" alt="image-20250705125954022"></p><p>①等温变化：<img src="file:///D:\TEMP\Temp\ksohtml13448\wps1.jpg" alt="img">，即<img src="file:///D:\TEMP\Temp\ksohtml13448\wps2.jpg" alt="img"></p><p>②绝热膨胀或压缩：<img src="file:///D:\TEMP\Temp\ksohtml13448\wps3.jpg" alt="img">，即<img src="file:///D:\TEMP\Temp\ksohtml13448\wps4.jpg" alt="img"></p><p>③等容变化：<img src="file:///D:\TEMP\Temp\ksohtml13448\wps5.jpg" alt="img">，即<img src="file:///D:\TEMP\Temp\ksohtml13448\wps6.jpg" alt="img"></p></li><li><p>功的计算（借助P-V图 图像面积计算功）</p><p>例<img src="C:\Users\王博文\AppData\Roaming\Typora\typora-user-images\image-20250705130622663.png" alt="image-20250705130622663"></p></li><li><p>介绍能量守恒定律</p><p>永动机无法制成 例子：小鸭饮水（环境热能）</p></li></ol><h2 id="7-22-液柱或活塞移动问题的讲解"><a href="#7-22-液柱或活塞移动问题的讲解" class="headerlink" title="7.22 液柱或活塞移动问题的讲解"></a>7.22 液柱或活塞移动问题的讲解</h2><h3 id="回顾复习"><a href="#回顾复习" class="headerlink" title="回顾复习"></a>回顾复习</h3><ol><li><p>理想气体状态方程：对于一定质量的理想气体，从某一状态变化到另一状态时，始终满足:</p><p><img src="C:\Users\王博文\AppData\Roaming\Typora\typora-user-images\image-20250705132651570.png" alt="image-20250705132651570"></p></li><li><p>热力学第一定律:  对于一个热力学系统而言，始终满足：<img src="C:\Users\王博文\AppData\Roaming\Typora\typora-user-images\image-20250705125954022.png" alt="image-20250705125954022"></p></li></ol><h3 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h3><p><img src="C:\Users\王博文\AppData\Roaming\Typora\typora-user-images\image-20250705130703309.png" alt="image-20250705130703309"></p><p>此题作为活塞类问题的引入，注重考生对于受力分析的掌握，同时多状态参量容易弄混，也需要P-V图的辅助，D选项的解题过程也用到了热力学第一定律</p><h3 id="液柱或活塞问题的分析"><a href="#液柱或活塞问题的分析" class="headerlink" title="液柱或活塞问题的分析"></a>液柱或活塞问题的分析</h3><ul><li>注重对于活塞处或液面处的受力分析（重力、压力、外力等）</li><li>对气体部分的分析：理想气体状态方程</li><li>适当画P-V图作辅助（对过程中状态参量的分析）</li><li>热力学第一定律的应用</li><li>注意：<ul><li>弹簧、器壁摩擦力造成的受力分析和能量分配上的影响</li><li>要小心液面高度和活塞高度是否有限制，比如容器容积的影响。</li></ul></li></ul><h3 id="例题讲解"><a href="#例题讲解" class="headerlink" title="例题讲解"></a>例题讲解</h3><p>【例1】**（23-24高二下·浙江舟山·期末）**固定在水平地面开口向上的圆柱形绝热气缸如图所示，用质量<img src="file:///D:\TEMP\Temp\ksohtml13448\wps16.png" alt="img">的绝热活塞密封一定质量的理想气体，活塞可以在气缸内无摩擦移动。活塞下端与气缸底部间连接一劲度系数<img src="file:///D:\TEMP\Temp\ksohtml13448\wps17.png" alt="img">的轻质弹簧。初始时，活塞与缸底的距离<img src="file:///D:\TEMP\Temp\ksohtml13448\wps18.png" alt="img">，弹簧正好处于原长，缸内气体温度<img src="file:///D:\TEMP\Temp\ksohtml13448\wps19.png" alt="img">。已知大气压强<img src="file:///D:\TEMP\Temp\ksohtml13448\wps20.png" alt="img">，活塞横截面积<img src="file:///D:\TEMP\Temp\ksohtml13448\wps21.png" alt="img">，不计弹簧的质量和体积。</p><p>（1）求初始状态时缸内气体的压强<img src="file:///D:\TEMP\Temp\ksohtml13448\wps22.png" alt="img">；</p><p>（2）现通过电热丝<em>R</em>加热缸内气体，使活塞缓慢上升4cm，求此时缸内气体的温度<img src="file:///D:\TEMP\Temp\ksohtml13448\wps23.png" alt="img">；</p><p>（3）在满足（2）的整个过程中，缸内气体的内能增加158J，求其吸收的热量<em>Q</em>。</p><p><img src="file:///D:\TEMP\Temp\ksohtml13448\wps24.jpg" alt="img"> </p><p>解析：</p><p><strong>（1）初态活塞处受力分析</strong></p><p><img src="file:///D:\TEMP\Temp\ksohtml13448\wps25.png" alt="img"></p><p>解得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps26.png" alt="img"> </p><p><strong>（2）加热后活塞处再次受力分析</strong></p><p>可得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps27.png" alt="img">  ①</p><p>解得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps28.png" alt="img">  <strong>（注意：弹力的影响)</strong></p><p>由理想气体状态方程<img src="file:///D:\TEMP\Temp\ksohtml13448\wps29.png" alt="img"></p><p>可得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps30.png" alt="img">②</p><p>解得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps31.png" alt="img"> </p><p><strong>（3）利用能量守恒</strong></p><p>注意弹簧弹性势能！！！</p><p><img src="file:///D:\TEMP\Temp\ksohtml13448\wps32.png" alt="img"></p><p>解得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps33.png" alt="img"></p><p>由热力学第一定律有<img src="file:///D:\TEMP\Temp\ksohtml13448\wps34.png" alt="img"></p><p>解得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps35.png" alt="img"></p><p>【例2】如图1所示，竖直玻璃管上端封闭、下端开口，总长<img src="https://staticzujuan.xkw.com/quesimg/Upload/formula/66593aa1cca5e785337470a85021496a.svg" alt="img">，横截面积<img src="https://staticzujuan.xkw.com/quesimg/Upload/formula/bbefa293522285cf59e805a717db5045.svg" alt="img">，管内液柱的长度<img src="https://staticzujuan.xkw.com/quesimg/Upload/formula/a23de2848a9874cbe1a868ddd5bd352b.svg" alt="img">，质量<img src="https://staticzujuan.xkw.com/quesimg/Upload/formula/40d029d40ab2e2cbbf3aba7295ac802a.svg" alt="img">，液柱密封一定质量的理想气体，气体的长度<img src="https://staticzujuan.xkw.com/quesimg/Upload/formula/e348232589d33d99cef12a4c2c849df9.svg" alt="img">，气体温度<img src="https://staticzujuan.xkw.com/quesimg/Upload/formula/4ea20fb7d996a19c2d22cd0d7db7e849.svg" alt="img">。现将玻璃管缓慢转到水平位置，气体温度仍为<img src="https://staticzujuan.xkw.com/quesimg/Upload/formula/4e9a724b59c890095baa5cb73e267c44.svg" alt="img">，气体长度变为<img src="https://staticzujuan.xkw.com/quesimg/Upload/formula/9fbd49bf20f987c05b4d36e31549075c.svg" alt="img">，如图2所示。然后对气体进行缓慢加热，使气体温度上升至<img src="https://staticzujuan.xkw.com/quesimg/Upload/formula/2caa4b8de24871e9d9cc05ad63b11aa7.svg" alt="img">，加热过程气体吸收热量<em>Q</em>，内能增加<img src="https://staticzujuan.xkw.com/quesimg/Upload/formula/38e96cd93aa0036ad420c2e0d902c211.svg" alt="img">，气体长度变为<img src="https://staticzujuan.xkw.com/quesimg/Upload/formula/87f93f4e10d9672fa6bd67243bc23d4a.svg" alt="img">，如图3所示。已知大气压强<img src="https://staticzujuan.xkw.com/quesimg/Upload/formula/b7092b2e953e0359983553acc79efa27.svg" alt="img">，玻璃管内壁光滑，重力加速度<img src="https://staticzujuan.xkw.com/quesimg/Upload/formula/46a6a294d3d7206bcdde5b943dbe94f0.svg" alt="img">。</p><p><img src="C:\Users\王博文\AppData\Roaming\Typora\typora-user-images\image-20250705135305201.png" alt="image-20250705135305201"></p><p>此题为液柱型问题，对于此类问题，要注意对液柱受力分析，此类问题相较活塞相对简单</p><h2 id="7-23-汽缸问题"><a href="#7-23-汽缸问题" class="headerlink" title="7.23 汽缸问题"></a>7.23 汽缸问题</h2><h3 id="回顾复习-1"><a href="#回顾复习-1" class="headerlink" title="回顾复习"></a>回顾复习</h3><ol><li>理想气体状态方程</li><li>热力学第一定律</li><li>液柱或活塞问题的分析：注意受力分析、热一、能量守恒的运用</li></ol><h3 id="例题讲解-1"><a href="#例题讲解-1" class="headerlink" title="例题讲解"></a>例题讲解</h3><p>【例1】活塞间相关联</p><ul><li>**（2022·河北石家庄·模拟预测）<em><em>如图所示。有一个竖直放置的圆筒，导热性能良好，两端开口且足够长，它由</em>ab<em>两段粗细不同的部分连接而成，横截面积分别为2</em>S*，<em>S</em>。两活塞AB的质量分别为2*m</em>，<em>m</em>。其中在两部分连接处有环形卡子EF，厚度不计，能保证活塞B不会运动到粗圆筒中。两活塞用长为2<em>l</em>不可伸长的轻绳相连，把一定质量理想气体密封在两活塞之间，活塞静止在图示位置，已知大气压强为<img src="file:///D:\TEMP\Temp\ksohtml13448\wps40.png" alt="img">，且<img src="file:///D:\TEMP\Temp\ksohtml13448\wps41.png" alt="img">。外界环境不变，忽略活塞与圆筒之间的摩擦。重力加速度为<em>g</em>。求</li></ul><p><img src="file:///D:\TEMP\Temp\ksohtml13448\wps42.jpg" alt="img"> </p><p>(1)图示位置轻绳拉力的大小<img src="file:///D:\TEMP\Temp\ksohtml13448\wps43.png" alt="img">；</p><p>(2)若剪断轻绳，求稳定后活塞B移动的距离；</p><p>(3)若不剪断细绳，自由释放整个装置（忽略空气阻力），求稳定后圆筒内气体的压强。</p><p><strong>这道题要注意活塞间的细绳，因为这部分关联，所以单独对活塞分析需要考虑拉力，此题也可考虑整体分析</strong></p><p>解答</p><p>设密封气体的压强为<img src="file:///D:\TEMP\Temp\ksohtml13448\wps44.png" alt="img">，对两活塞整体受力分析如图1所示，由平衡条件可得</p><p><img src="file:///D:\TEMP\Temp\ksohtml13448\wps45.jpg" alt="img"> </p><p><img src="file:///D:\TEMP\Temp\ksohtml13448\wps46.png" alt="img"></p><p>对活塞B受力分析，如图2所示，由平衡条件得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps47.png" alt="img"></p><p>联立求得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps48.png" alt="img">，<img src="file:///D:\TEMP\Temp\ksohtml13448\wps49.png" alt="img"></p><p>（2）设剪断轻绳后，最终稳定时密封气体的压强为<img src="file:///D:\TEMP\Temp\ksohtml13448\wps50.png" alt="img">，以活塞B为研究对象可得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps51.png" alt="img"></p><p>得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps52.png" alt="img"></p><p>对活塞B受力分析，显然<img src="file:///D:\TEMP\Temp\ksohtml13448\wps53.png" alt="img"></p><p>所以，活塞A将下落到卡子处。以密封气体为研究对象，由玻意耳定律可得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps54.png" alt="img"></p><p>得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps55.png" alt="img"></p><p>设稳定后活塞B移动的距离为<img src="file:///D:\TEMP\Temp\ksohtml13448\wps56.png" alt="img">，则<img src="file:///D:\TEMP\Temp\ksohtml13448\wps57.png" alt="img"></p><p>求得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps58.png" alt="img"></p><p>（3）若不剪断细绳，自由释放整个装置，设稳定后圆筒内气体的压强为<img src="file:///D:\TEMP\Temp\ksohtml13448\wps59.png" alt="img">，对两活塞整体受力分析，根据牛顿第二定律有<img src="file:///D:\TEMP\Temp\ksohtml13448\wps60.png" alt="img"></p><p>求得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps61.png" alt="img"></p><p>【例2】容器容积限制</p><ul><li><p>**（2025·湖南长沙·一模）**为了测量一些形状不规则而又不便浸入液体的固体体积，小星同学用气压计连通一个带有密封门的导热汽缸做成如图所示的装置，汽缸中<img src="file:///D:\TEMP\Temp\ksohtml13448\wps73.png" alt="img">两处设有固定卡环，厚度可忽略的密封良好的活塞可在其间运动。已知卡环<img src="file:///D:\TEMP\Temp\ksohtml13448\wps74.png" alt="img">下方汽缸的容积为<img src="file:///D:\TEMP\Temp\ksohtml13448\wps75.png" alt="img">，外界温度恒定，大气压强为<img src="file:///D:\TEMP\Temp\ksohtml13448\wps76.png" alt="img">，忽略气压计管道的容积。</p><p><img src="file:///D:\TEMP\Temp\ksohtml13448\wps77.jpg" alt="img"> </p><p>(1)打开密封门，将活塞放至卡环<img src="file:///D:\TEMP\Temp\ksohtml13448\wps78.png" alt="img">处，然后关闭密封门，将活塞从卡环<img src="file:///D:\TEMP\Temp\ksohtml13448\wps79.png" alt="img">处缓慢拉至卡环<img src="file:///D:\TEMP\Temp\ksohtml13448\wps80.png" alt="img">处，此时气压计的示数<img src="file:///D:\TEMP\Temp\ksohtml13448\wps81.png" alt="img">，求活塞在<img src="file:///D:\TEMP\Temp\ksohtml13448\wps82.png" alt="img">处时气缸的容积<img src="file:///D:\TEMP\Temp\ksohtml13448\wps83.png" alt="img">；</p><p>(2)打开密封门，将待测固体<img src="file:///D:\TEMP\Temp\ksohtml13448\wps84.png" alt="img">放入气缸中，将活塞放至卡环<img src="file:///D:\TEMP\Temp\ksohtml13448\wps85.png" alt="img">处，然后关闭密封门，将活塞从卡环<img src="file:///D:\TEMP\Temp\ksohtml13448\wps86.png" alt="img">处缓慢拉至卡环<img src="file:///D:\TEMP\Temp\ksohtml13448\wps87.png" alt="img">处，此时气压计的示数<img src="file:///D:\TEMP\Temp\ksohtml13448\wps88.png" alt="img">，求待测物体<img src="file:///D:\TEMP\Temp\ksohtml13448\wps89.png" alt="img">的体积<img src="file:///D:\TEMP\Temp\ksohtml13448\wps90.png" alt="img">。</p></li></ul><p><strong>本题要小心容器容积有限并且待测物A体积未知的影响，但是气体始终保持等温变化，所以可以利用理想气体状态方程来求解</strong></p><p>解析</p><p>（1）因气缸导热且外界温度恒定，则封闭气体发生等温膨胀，由理想气体状态方程有<img src="file:///D:\TEMP\Temp\ksohtml13448\wps96.png" alt="img"></p><p>解得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps97.png" alt="img"></p><p>（2）放入待测固体<img src="file:///D:\TEMP\Temp\ksohtml13448\wps98.png" alt="img">后封闭，再活塞从卡环<img src="file:///D:\TEMP\Temp\ksohtml13448\wps99.png" alt="img">处缓慢拉至卡环<img src="file:///D:\TEMP\Temp\ksohtml13448\wps100.png" alt="img">处，封闭气体依然发生等温膨胀，有<img src="file:///D:\TEMP\Temp\ksohtml13448\wps101.png" alt="img"></p><p>联立解得<img src="file:///D:\TEMP\Temp\ksohtml13448\wps102.png" alt="img"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>对于汽缸中的活塞问题，一定要做好受力分析，此外题目中的限制条件（如外力、容器体积受限、附带弹簧细绳等）都需要注意，核心是要充分掌握理想气体状态方程、热力学第一定律、能量守恒定律，在此基础上多加练习。</p>]]></content>
    
    
    <categories>
      
      <category>杂</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>西方美学导论</title>
    <link href="/2025/07/08/%E8%A5%BF%E6%96%B9%E7%BE%8E%E5%AD%A6%E5%AF%BC%E8%AE%BA%EF%BC%88%E6%95%A3%E8%A3%85%EF%BC%89/"/>
    <url>/2025/07/08/%E8%A5%BF%E6%96%B9%E7%BE%8E%E5%AD%A6%E5%AF%BC%E8%AE%BA%EF%BC%88%E6%95%A3%E8%A3%85%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="第十讲-从柏克到康德：美与崇高"><a href="#第十讲-从柏克到康德：美与崇高" class="headerlink" title="第十讲 从柏克到康德：美与崇高"></a>第十讲 从柏克到康德：美与崇高</h2><h3 id="1、柏克对美与崇高的分析"><a href="#1、柏克对美与崇高的分析" class="headerlink" title="1、柏克对美与崇高的分析"></a>1、柏克对美与崇高的分析</h3><p>激情或情感的分析</p><p>激情：痛苦、愉悦</p><p>美：不在于比例、效用与适合、完善、德性</p><p>崇高：令人恐惧的事物</p><p>语言的崇高（杜博神父《对诗与画的批判式思考》）：绘画通过视觉、无需人为设定符号</p><p>——&gt;绘画是有打动人的力量</p><p><strong>柏克两大文学崇高形象：1、钦定版《圣经》2、弥尔顿《失乐园》</strong></p><p>柏克认为：诗歌有更有力的力量来支配情感</p><p><strong>理性主义认为</strong>：<strong>越清晰的东西越美</strong></p><p><strong>柏克：政治保守主义</strong></p><p><strong>韩炳哲</strong>：<strong>当今时代的</strong>“<strong>平滑之美</strong>“<strong>与对痛苦的逃避</strong>**</p><h3 id="2、康德对美与崇高的分析"><a href="#2、康德对美与崇高的分析" class="headerlink" title="2、康德对美与崇高的分析"></a>2、康德对美与崇高的分析</h3><h4 id="一、"><a href="#一、" class="headerlink" title="一、"></a>一、</h4><p>1、前批判时期、<strong>批判时期《纯粹理性批判》、《实践理性批判》、《判断力批判》</strong>、晚期</p><p>2、<strong>判断力（1）规定性的判断力（2）反思性的判断力</strong></p><p>3、我能够知道什么（对客观世界的认识）</p><h4 id="二、对美的分析"><a href="#二、对美的分析" class="headerlink" title="二、对美的分析"></a>二、对美的分析</h4><p>4、他致力于寻找品味的先天法则、而非经验法则</p><p>5、<strong>第一契机：鉴赏通过不带任何利害的愉悦对一个对象作出评判，愉悦对象叫作美</strong>  去功利化的愉悦</p><p>6、<strong>第二契机：美是没有概念而普遍令人喜欢的东西</strong></p><p>7、<strong>第三契机：美是一个对象的合目的性的形式</strong> 无感官、无目的性</p><p>8、<strong>第四契机：美没有概念而被认作必然愉悦</strong></p><p>9、鉴赏：知性与想象的自由游戏</p><h4 id="三、对崇高的分析"><a href="#三、对崇高的分析" class="headerlink" title="三、对崇高的分析"></a>三、对崇高的分析</h4><p>数字的崇高、力学的崇高</p><p>崇高（消极的愉快）促进生命的情感、美是一种间接产生的愉悦</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>方法论相关的（？）</title>
    <link href="/2025/07/06/idea/"/>
    <url>/2025/07/06/idea/</url>
    
    <content type="html"><![CDATA[<p>1、心理学驱动的专注力应用</p><p><a href="https://github.com/KenXiao1/momentum">KenXiao1&#x2F;momentum</a></p>]]></content>
    
    
    <categories>
      
      <category>一些思考</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>保研</title>
    <link href="/2025/07/06/%E4%BF%9D%E7%A0%94%E6%94%BF%E7%AD%96/"/>
    <url>/2025/07/06/%E4%BF%9D%E7%A0%94%E6%94%BF%E7%AD%96/</url>
    
    <content type="html"><![CDATA[<p>1、一些网站</p><p>（1）保研</p><p><a href="https://csbaoyan.top/">计算机保研指南 | CSWiki</a></p><ul><li>20清华徐绍俊</li></ul><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250813144116744.png" alt="image-20250813144116744"></p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250813145105315.png" alt="image-20250813145105315"></p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250813145426266.png" alt="image-20250813145426266"></p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250813145923202.png" alt="image-20250813145923202"></p><ul><li>20陈佳禾——研究所</li></ul><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250813150846893.png" alt="image-20250813150846893"></p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250813151036853.png" alt="image-20250813151036853"></p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250813151348470.png" alt="image-20250813151348470"></p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250813151523633.png" alt="image-20250813151523633"></p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250813152008134.png" alt="image-20250813152008134"></p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250813153215639.png" alt="image-20250813153215639"></p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250813153401788.png" alt="image-20250813153401788"></p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250813153809401.png" alt="image-20250813153809401"></p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250813154015275.png" alt="image-20250813154015275"></p><ul><li>20刘芃骐（光电）——本校</li></ul><p>大三聊了七个光电学院的老师（先发邮件（写邮件：靠deepseek 再修一修）：做一下前期沟通 办公室敲门）</p><p>找导师：</p><p>1、了解情况</p><p>2、参观组会、参观实验室、和学长学姐沟通</p><p>大三下已经进组</p><p>学会向上社交</p><p>硕转博（后路 但是会占两个名额）</p><p>博转硕（浙大光电暂时可以）</p><p>本校保无科研经历要求</p><p>保研流程：1、预推免（有志愿顺序 这个时候就已经需要确定导师）2、组织面试</p><p>各种组可以双开 尝试一下不同方向</p><p>完整的科研经历 不一定需要依靠文章 更好的方式是 推荐信</p><p>国内更看重实习（肉身&#x2F;远程）</p><p><strong>注意时间点</strong></p><p>课题组和老师优于title和学院</p><p>选导师时：</p><p>1、某些公众号 看毕业生去向</p><p>2、不是博后挂了很多一作 小心</p><p>3、搞清读博的需求？</p><p>稍稍关注一下清华苏世民书院</p><p>上海创智学院&#x2F;国家人工智能学院（上海）（生源可能有点拉）</p><p>问：</p><p>1、如何联系清北的导师？去官网找？实习机会？</p><p>2、夏令营相关问题？committee</p><p>3、phd有工资？</p><p>4、gap???</p><p>5、导师帽子大4050？</p><p>（2）暑研申请</p><p>1、去网站看老师介绍和邮箱</p><p>2、问学长学姐组的情况</p><p>3、 </p><p>（3）留学申请</p><p><a href="https://www.1point3acres.com/">一亩三分地社区: 留学|求职|投资|移民|生活 - 高信噪比 + 纯干货</a></p><p><a href="https://www.thegradcafe.com/">The GradCafe</a></p>]]></content>
    
    
    <categories>
      
      <category>升学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记-test</title>
    <link href="/2025/07/06/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-test/"/>
    <url>/2025/07/06/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-test/</url>
    
    <content type="html"><![CDATA[<p>just test…</p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>51单片机</title>
    <link href="/2025/07/05/51%E5%8D%95%E7%89%87%E6%9C%BA/"/>
    <url>/2025/07/05/51%E5%8D%95%E7%89%87%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<p>型号：STC89C52RC&#x2F;LE52RC这个型号</p><p>Keil上选型号：Atmel-AT89C52</p><h1 id="Lecture-2-LED"><a href="#Lecture-2-LED" class="headerlink" title="Lecture 2 LED"></a>Lecture 2 LED</h1><h2 id="2-1-点亮一个LED"><a href="#2-1-点亮一个LED" class="headerlink" title="2.1 点亮一个LED"></a>2.1 点亮一个LED</h2><p>贴片二极管：绿色为负极</p><p>电阻读数：</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250901181105938.png" alt="image-20250901181105938"></p><p>单片机里CPU通过程序控制寄存器，寄存器再通过驱动器实现对应功能</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250901181600015.png" alt="image-20250901181600015"></p><p>进制转换</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250901183337379.png" alt="image-20250901183337379"></p><p>LED  0xFE 11111110（末位D1亮）</p><h2 id="2-2-可控时长流水灯"><a href="#2-2-可控时长流水灯" class="headerlink" title="2.2 可控时长流水灯"></a>2.2 可控时长流水灯</h2><p>延时操作：stc-isp中软件延时计算器</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250901190647834.png" alt="image-20250901190647834"></p><p>可控延时函数 延时1ms</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;INTRINS.H&gt;</span></span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">Delay</span><span class="hljs-params">(<span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> xms)</span><span class="hljs-comment">//@12.000MHz</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">unsigned</span> <span class="hljs-type">char</span> i, j;<br><span class="hljs-keyword">while</span>(xms--)<br>&#123;<br>i = <span class="hljs-number">2</span>;<br>j = <span class="hljs-number">239</span>;<br><span class="hljs-keyword">do</span><br>&#123;<br><span class="hljs-keyword">while</span> (--j);<br>&#125; <span class="hljs-keyword">while</span> (--i);<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="Lecture-3-独立按键"><a href="#Lecture-3-独立按键" class="headerlink" title="Lecture 3 独立按键"></a>Lecture 3 独立按键</h1><h2 id="3-独立按键控制LED移位"><a href="#3-独立按键控制LED移位" class="headerlink" title="3 独立按键控制LED移位"></a>3 独立按键控制LED移位</h2><p>消抖操作：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs scss">int <span class="hljs-selector-tag">main</span>()&#123;<br><span class="hljs-built_in">while</span>(<span class="hljs-number">1</span>)<br>&#123;<br><span class="hljs-built_in">if</span>(P3_1==<span class="hljs-number">0</span>)<br>&#123;<br><span class="hljs-built_in">Delay</span>(<span class="hljs-number">20</span>);<br><span class="hljs-built_in">while</span>(P3_1==<span class="hljs-number">0</span>);<br><span class="hljs-built_in">Delay</span>(<span class="hljs-number">20</span>);<br>P2--;<br>&#125;<br>&#125;<br> &#125;<br></code></pre></td></tr></table></figure><p>其中的Delay(20)用于消抖延时</p><p><code>while(P3_1==0);</code>按下状态时始终执行空语句，用于等待按键释放的操作</p><p>否则按键按下时LED会一直闪</p><p>if语句和P2–构成了按键控制LED亮灭</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs routeros">int main()<br>&#123;<br>char <span class="hljs-attribute">lednum</span>=0x00;<br><span class="hljs-attribute">P2</span>=~0x01;<br><span class="hljs-keyword">while</span>(1)<br>&#123;<br><span class="hljs-keyword">if</span>(<span class="hljs-attribute">P3_1</span>==0)<br>&#123;<br>Delay(20);<br><span class="hljs-keyword">while</span>(<span class="hljs-attribute">P3_1</span>==0);<br>Delay(20);<br>lednum++;<br><span class="hljs-keyword">if</span>(lednum&gt;=8)<span class="hljs-attribute">lednum</span>=0;<br><span class="hljs-attribute">P2</span>=~(0x01&lt;&lt;lednum);<br>&#125;<br><br><span class="hljs-keyword">if</span>(<span class="hljs-attribute">P3_0</span>==0)&#123;<br>Delay(20);<br><span class="hljs-keyword">while</span>(<span class="hljs-attribute">P3_0</span>==0);<br>Delay(20);<br>lednum--;<br><span class="hljs-keyword">if</span>(lednum&lt;0)<span class="hljs-attribute">lednum</span>=7;<br><span class="hljs-attribute">P2</span>=~(0x01&lt;&lt;lednum);<br>&#125;<br>&#125;<br> &#125;<br></code></pre></td></tr></table></figure><p>两个对于lednum的操作是用于处理二进制溢出的问题，实现满8归0</p><p>取反是因为P2为低电平（0）时才亮</p><h1 id="Lecture-4-数码管"><a href="#Lecture-4-数码管" class="headerlink" title="Lecture 4 数码管"></a>Lecture 4 数码管</h1><h2 id="4-1-静态数码管显示"><a href="#4-1-静态数码管显示" class="headerlink" title="4.1 静态数码管显示"></a>4.1 静态数码管显示</h2><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250902110711424.png" alt="image-20250902110711424"></p><p>上面为共阴极接地，下面为共阳极接VCC</p><p>原理图分析：</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250902113227203.png" alt="image-20250902113227203"></p><p>P22 P23 P24均为输入端 8个LED端口为输出端</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250902113436698.png" alt="image-20250902113436698"></p><p>C端为高端 通过CBA的十进制数保证Yx为低电平</p><p>现在目标让第三个输出6</p><p>得先让LED6为0 也就是上面74LS138 LED6为输出 也就是Y5 也就是CBA二进制输入为5</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250902115420847.png" alt="image-20250902115420847"></p><h2 id="4-2-动态数码管显示"><a href="#4-2-动态数码管显示" class="headerlink" title="4.2 动态数码管显示"></a>4.2 动态数码管显示</h2><p>核心思想：利用极短暂延时 人眼无法察觉使得同时显示多个数字</p><p>要注意消影问题</p><p>利用清零操作</p><p>位选 段选 **清零 ** 位选 段选</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250902151836231.png" alt="image-20250902151836231"></p><p>上面在Nixie函数中的延时＋归零 避免重影</p><p>下面直接的调用</p><h1 id="Lecture-5-调试"><a href="#Lecture-5-调试" class="headerlink" title="Lecture 5 调试"></a>Lecture 5 调试</h1><h2 id="Lecture-5-1-模块化编程"><a href="#Lecture-5-1-模块化编程" class="headerlink" title="Lecture 5.1 模块化编程"></a>Lecture 5.1 模块化编程</h2><p>模块化编程：记得放.c和.h</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250902154020130.png"></p><p>根目录下加两个文件：Delay.c Delay.h</p><p>小心不要写成Dealy了！！！！</p><p>Delay.c</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">Delay</span><span class="hljs-params">(<span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> xms)</span>        <span class="hljs-comment">//@12.000MHz</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">char</span> i, j;<br>    <span class="hljs-keyword">while</span>(xms--)<br>    &#123;<br>        i = <span class="hljs-number">2</span>;<br>        j = <span class="hljs-number">239</span>;<br>        <span class="hljs-keyword">do</span><br>        &#123;<br>            <span class="hljs-keyword">while</span> (--j);<br>        &#125; <span class="hljs-keyword">while</span> (--i);<br>    &#125;<br>&#125;<span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;Delay.h&quot;</span></span><br></code></pre></td></tr></table></figure><p>Delay.h文件里面的调用</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">ifndef</span> __DELAY_H__</span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> __DELAY_H__</span><br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">Delay</span><span class="hljs-params">(<span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> xms)</span></span>;  <span class="hljs-comment">// 函数声明</span><br><br><span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br><br></code></pre></td></tr></table></figure><p>main函数里面记得引用回来  <code>#include &quot;Delay.h&quot;</code></p><h2 id="Lecture-5-2-LCD1602调试工具"><a href="#Lecture-5-2-LCD1602调试工具" class="headerlink" title="Lecture 5.2 LCD1602调试工具"></a>Lecture 5.2 LCD1602调试工具</h2><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250902152326398.png" alt="image-20250902152326398"></p><h1 id="Lecture-6-矩阵键盘"><a href="#Lecture-6-矩阵键盘" class="headerlink" title="Lecture 6 矩阵键盘"></a>Lecture 6 矩阵键盘</h1><h2 id="6-1矩阵键盘的读取"><a href="#6-1矩阵键盘的读取" class="headerlink" title="6.1矩阵键盘的读取"></a>6.1矩阵键盘的读取</h2><h3 id="6-1-1扫描"><a href="#6-1-1扫描" class="headerlink" title="6.1.1扫描"></a>6.1.1扫描</h3><p>数码管扫描（输出扫描）原理：显示第1位→显示第2位→显示第3位→……，然后快速循环这个过程，最终实现所有数码管同时显示的效果矩阵键盘扫描（输入扫描）原理：读取第1行(列)→读取第2行(列) →读取第3行(列) → ……，然后快速循环这个过程，最终实现所有按键同时检测的效果以上两种扫描方式的共性：节省I&#x2F;O口</p><p><img src="C:\Users\fluorine\AppData\Roaming\Typora\typora-user-images\image-20250902174712091.png" alt="image-20250902174712091"></p><p>矩阵扫描利用按列扫描方式</p><p>错误调用</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs scss">void <span class="hljs-selector-tag">main</span>()&#123;<br><span class="hljs-built_in">LCD_Init</span>();<br><span class="hljs-built_in">while</span>(<span class="hljs-number">1</span>)<br>&#123;<br><span class="hljs-built_in">LCD_ShowNum</span>(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,Matrixkey(),<span class="hljs-number">3</span>);<br>&#125;<br><br></code></pre></td></tr></table></figure><ul><li><strong>每微秒调用一次</strong> Matrixkey()（非常频繁）</li><li><strong>即使没有按键</strong>，也会在屏幕上显示”000”</li><li><strong>屏幕疯狂闪烁</strong>，根本看不清内容</li><li><strong>当真的有按键时</strong>，可能因为刷新太快而错过显示</li></ul><p>正确调用</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs processing"><span class="hljs-keyword">while</span>(<span class="hljs-number">1</span>)<br>&#123;<br><span class="hljs-type">char</span> <span class="hljs-built_in">key</span> = <span class="hljs-title function_">Matrixkey</span>();<br><span class="hljs-keyword">if</span>(<span class="hljs-built_in">key</span>)<br>&#123;<br><span class="hljs-title function_">LCD_ShowNum</span>(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-built_in">key</span>,<span class="hljs-number">2</span>);<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>要有一个 <code>if(key)</code>的判断 避免速度过快 疯狂调用 显示00 if的目的是锁住显示函数 </p><p>不加判断的话 他会迅速变成00</p><h2 id="6-2-矩阵键盘密码锁"><a href="#6-2-矩阵键盘密码锁" class="headerlink" title="6.2 矩阵键盘密码锁"></a>6.2 矩阵键盘密码锁</h2><p>贴一下源码 注意一下几个条件判断以及结果的复位清零操作</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;REGX52.H&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;Delay.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;LCD1602.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;Matrixkey.h&quot;</span></span><br><br><span class="hljs-comment">// 1-9为基础密码 10为密码&quot;0&quot; 11为确认 12为取消 13-16无用处</span><br><span class="hljs-type">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span>&#123;<br>    <span class="hljs-type">int</span> k, num = <span class="hljs-number">0</span>;<br>    <span class="hljs-type">int</span> Password = <span class="hljs-number">1206</span>;<br>    <span class="hljs-type">char</span> key;<br>    <span class="hljs-type">int</span> inputComplete = <span class="hljs-number">0</span>;  <span class="hljs-comment">// 添加标志位，表示输入是否完成</span><br>    <br>    LCD_Init();<br>    <br>    <span class="hljs-keyword">while</span>(<span class="hljs-number">1</span>)<br>    &#123;<br>        <span class="hljs-keyword">if</span>(!inputComplete)  <span class="hljs-comment">// 如果输入未完成，显示密码输入界面</span><br>        &#123;<br>            LCD_ShowString(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-string">&quot;Password:       &quot;</span>);  <span class="hljs-comment">// 使用空格清除整行</span><br>            LCD_ShowString(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-string">&quot;                &quot;</span>);  <span class="hljs-comment">// 清除第二行</span><br>            <br>            key = Matrixkey();<br>            <br>            <span class="hljs-keyword">if</span>(key)<br>            &#123;<br>                <span class="hljs-keyword">if</span>(key &lt;= <span class="hljs-number">10</span>)<br>                &#123;<br>                    k = key % <span class="hljs-number">10</span>;<br>                    num = num * <span class="hljs-number">10</span> + k;<br>                    LCD_ShowNum(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, num, <span class="hljs-number">4</span>);<br>                &#125;<br>                <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(key == <span class="hljs-number">11</span>)  <span class="hljs-comment">// 确认键</span><br>                &#123;<br>                    inputComplete = <span class="hljs-number">1</span>;  <span class="hljs-comment">// 标记输入完成</span><br>                &#125;<br>                <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(key == <span class="hljs-number">12</span>)  <span class="hljs-comment">// 取消键</span><br>                &#123;<br>                    num = <span class="hljs-number">0</span>;  <span class="hljs-comment">// 清零</span><br>                    LCD_ShowNum(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, num, <span class="hljs-number">4</span>);<br>                &#125;<br>                <br>                Delay_ms(<span class="hljs-number">100</span>);<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">else</span>  <span class="hljs-comment">// 输入完成后显示验证结果</span><br>        &#123;<br>            <span class="hljs-comment">// 清除整个屏幕</span><br>            LCD_ShowString(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-string">&quot;                &quot;</span>);  <span class="hljs-comment">// 第一行清空</span><br>            LCD_ShowString(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-string">&quot;                &quot;</span>);  <span class="hljs-comment">// 第二行清空</span><br>            <br>            <span class="hljs-keyword">if</span>(num == Password)<br>            &#123;<br>                LCD_ShowString(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-string">&quot;***Congratulations!***&quot;</span>);<br>            &#125;<br>            <span class="hljs-keyword">else</span><br>            &#123;<br>                LCD_ShowString(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-string">&quot;***Don&#x27;t give up!***&quot;</span>);<br>            &#125;<br>            <br>            <span class="hljs-comment">// 添加延时，然后重置状态</span><br>            Delay_ms(<span class="hljs-number">2000</span>);<br>            num = <span class="hljs-number">0</span>;<br>            inputComplete = <span class="hljs-number">0</span>;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>课程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>my blog</title>
    <link href="/2025/07/05/my-blog/"/>
    <url>/2025/07/05/my-blog/</url>
    
    <content type="html"><![CDATA[<p>序章…（以后有机会补）</p>]]></content>
    
    
    <categories>
      
      <category>杂</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
